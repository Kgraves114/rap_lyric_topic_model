{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# This notebook performs topic modeling on lyrics to that we can investigate questions including : The differences between East Coast and West Coast rap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# common Python imports\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# enable plotting in our notebook\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# these are used for NLP, Data Manipulation, etc\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load some NLTK data before we get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\slick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Wall time: 1.45 s\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\slick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Wall time: 20 ms\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\slick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Wall time: 68.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time nltk.download('punkt')\n",
    "%time nltk.download('stopwords')\n",
    "%time nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# And load a part-of-speech tagging model that was already trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# helper function to translate POS tags from treebank to wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tag.brill.BrillTagger object at 0x0000026BF9B54C88>\n",
      "[('The', 'DT'), ('cat', '-None-'), ('walked', 'VBD'), ('onto', 'IN'), ('an', 'DT'), ('airplane', 'NN')]\n",
      "['The', 'cat', 'walk', 'onto', 'an', 'airplane']\n"
     ]
    }
   ],
   "source": [
    "# set up our lemmatizer in case we enable it\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# and load a POS tagger\n",
    "# let's also load and test a Brill Part of Speech tagger which was trained on the Penn Treebank:\n",
    "BRILL_TAGGER_FILE_PATH = 'resources/treebank_brill_aubt.pickle'\n",
    "brill_tagger = pickle.load(open(BRILL_TAGGER_FILE_PATH, 'rb'))\n",
    "print(brill_tagger)\n",
    "\n",
    "# now let's kick the tires on this tagger\n",
    "test_tag_tokens = 'The cat walked onto an airplane'.split()\n",
    "print(brill_tagger.tag(test_tag_tokens))\n",
    "\n",
    "print([lemma.lemmatize(x[0], get_wordnet_pos(x[1])) for x in brill_tagger.tag(test_tag_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Get', '-None-'), ('me', 'PRP'), ('on', 'IN'), ('the', 'DT'), ('court', 'NN'), ('and', 'CC'), (\"I'm\", '-None-'), ('trouble', 'NN'), ('Last', 'JJ'), ('week', 'NN'), ('fucked', 'VBD'), ('around', 'IN'), ('and', 'CC'), ('got', 'VBD'), ('a', 'DT'), ('triple', 'RB'), ('double', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "test_sentence_2 = 'Get me on the court and I\\'m trouble Last week fucked around and got a triple double'\n",
    "print(brill_tagger.tag(test_sentence_2.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load some stopwords -- words which are commonly filtered out since they are common or do not carry much meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'other', 'myself', 'should', 'between', 'being', 'your', \"shan't\", \"mightn't\", 'her', 'didn', 'here', 'to', 'the', 'while', 'after', 'such', 're', 've', 'haven', 'who', 'hadn', 'had', 'some', 'y', 'below', 'hasn', 'can', 'be', 'don', 'where', 'yourselves', 'm', 'most', 'weren', \"didn't\", 'through', 'as', 'did', \"wouldn't\", 't', 'whom', 'if', 'doesn', \"don't\", 'was', 'mightn', 'how', 'ours', \"you've\", 'd', 'under', 'yours', 'won', 'each', 'about', \"you'll\", 'because', 'in', 'own', 'you', 'have', 'wouldn', 'him', 'down', 'ain', \"hadn't\", 'are', 's', \"should've\", 'during', 'more', 'we', 'that', \"haven't\", 'again', 'why', 'herself', 'very', \"you're\", 'they', 'their', 'does', 'not', 'our', 'needn', 'too', 'its', 'she', 'no', 'ourselves', 'but', 'against', 'before', 'there', 'been', 'he', 'from', 'so', \"won't\", 'those', 'what', \"that'll\", 'it', 'with', 'ma', 'above', 'of', 'few', \"needn't\", 'further', \"couldn't\", 'i', 'o', 'hers', \"weren't\", 'these', 'mustn', 'shouldn', 'and', 'themselves', \"isn't\", 'has', \"wasn't\", 'himself', 'when', 'any', 'shan', \"you'd\", 'a', 'will', 'doing', 'theirs', 'on', 'at', \"shouldn't\", 'yourself', 'then', 'were', 'couldn', 'just', 'wasn', \"hasn't\", 'off', 'nor', 'up', 'is', 'me', 'this', 'll', \"it's\", 'same', 'isn', 'aren', 'by', 'or', 'only', 'my', 'now', 'am', 'them', 'once', 'which', \"she's\", \"doesn't\", 'having', 'his', 'than', \"mustn't\", 'itself', 'into', 'do', \"aren't\", 'both', 'an', 'for', 'over', 'out', 'until', 'all'}\n"
     ]
    }
   ],
   "source": [
    "# load a stopword set we want to use...\n",
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "stop_word_set = set(stoplist)\n",
    "\n",
    "# we'll remove a few more from our dataset\n",
    "if False:\n",
    "    stop_word_set.add('get')\n",
    "    stop_word_set.add('got')\n",
    "    stop_word_set.add('nigga')\n",
    "    stop_word_set.add('niggas')\n",
    "    stop_word_set.add('bitch')\n",
    "    stop_word_set.add('fuck')\n",
    "    stop_word_set.add('ain\\t')\n",
    "    stop_word_set.add('aint')\n",
    "\n",
    "print(stop_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loading the dataset\n",
    "## This dataset comes from the Kaggle website at this URL: https://www.kaggle.com/artimous/every-song-you-have-heard-almost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframes from CSV.  This might take some time...\n",
      "Length of Set #1 : 250000\n",
      "Length of Set #2 : 266174\n",
      "Length of Both Set combined : 516174\n",
      "Wall time: 36.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Loading dataframes from CSV.  This might take some time...')\n",
    "\n",
    "# NOTE : Without setting the engine here, we might hit the exception : \"C error: EOF inside string ...\"\n",
    "\n",
    "# This dataset is comprised of two separate files possibly for size and download limitations\n",
    "# so we'll put them together in a moment...\n",
    "lyrics_1_df = pd.read_csv('c:/datasets/lyrics/lyrics1.csv',\n",
    "                       engine = 'python')\n",
    "lyrics_2_df = pd.read_csv('c:/datasets/lyrics/lyrics2.csv',\n",
    "                       engine = 'python')\n",
    "# now we can put them together into a single frame\n",
    "lyrics_df = pd.concat([lyrics_1_df, lyrics_2_df])\n",
    "\n",
    "print('Length of Set #1 : {}'.format(len(lyrics_1_df)))\n",
    "print('Length of Set #2 : {}'.format(len(lyrics_2_df)))\n",
    "print('Length of Both Set combined : {}'.format(len(lyrics_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#replace carriage returns with periods to see if we can split lyrics as if they are sentences\n",
    "lyrics_df = lyrics_df.replace({'\\n': ' . '}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Band</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>No, no . I ain't ever trapped out the bando . ...</td>\n",
       "      <td>Everyday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>The drinks go down and smoke goes up, I feel m...</td>\n",
       "      <td>Live Till We Die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>She don't live on planet Earth no more . She f...</td>\n",
       "      <td>The Otherside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>Trippin' off that Grigio, mobbin', lights low ...</td>\n",
       "      <td>Pinot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I see a midnight panther, so gallant and so br...</td>\n",
       "      <td>Shadows &amp; Diamonds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I just want to ready your mind . 'Cause I'll s...</td>\n",
       "      <td>Uno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elijah Harris</td>\n",
       "      <td>To believe . Or not to believe . That is the q...</td>\n",
       "      <td>Girlfriend (Main)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Elijah Levi</td>\n",
       "      <td>No one here can love or understand me . Oh, wh...</td>\n",
       "      <td>Bye Bye Blackbird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Elijah Levi</td>\n",
       "      <td>Lullaby of Birdland, that's what I  . Always h...</td>\n",
       "      <td>Lullaby of Birdland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elijah Levi</td>\n",
       "      <td>I hate to see that evening sun go down . I hat...</td>\n",
       "      <td>St. Louis Blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Band                                             Lyrics  \\\n",
       "0   Elijah Blake  No, no . I ain't ever trapped out the bando . ...   \n",
       "1   Elijah Blake  The drinks go down and smoke goes up, I feel m...   \n",
       "2   Elijah Blake  She don't live on planet Earth no more . She f...   \n",
       "3   Elijah Blake  Trippin' off that Grigio, mobbin', lights low ...   \n",
       "4   Elijah Blake  I see a midnight panther, so gallant and so br...   \n",
       "5   Elijah Blake  I just want to ready your mind . 'Cause I'll s...   \n",
       "6  Elijah Harris  To believe . Or not to believe . That is the q...   \n",
       "7    Elijah Levi  No one here can love or understand me . Oh, wh...   \n",
       "8    Elijah Levi  Lullaby of Birdland, that's what I  . Always h...   \n",
       "9    Elijah Levi  I hate to see that evening sun go down . I hat...   \n",
       "\n",
       "                  Song  \n",
       "0             Everyday  \n",
       "1     Live Till We Die  \n",
       "2        The Otherside  \n",
       "3                Pinot  \n",
       "4   Shadows & Diamonds  \n",
       "5                  Uno  \n",
       "6    Girlfriend (Main)  \n",
       "7    Bye Bye Blackbird  \n",
       "8  Lullaby of Birdland  \n",
       "9      St. Louis Blues  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Before we start do do any text analysis, let's figure out the Hip-Hop artists we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# NOTE : Could not find the following in this set : \n",
    "# EAST COAST : Nas\n",
    "# WEST COAST : Warren G, Tha Dogg Pound\n",
    "east_coast_artists = ['The Notorious B.I.G.', 'Diddy', 'Wu-Tang Clan', 'Craig Mack', 'Tim Dog']\n",
    "west_coast_artists = ['N.W.A', 'Dr. Dre', '2Pac', 'Eazy-E', 'Ice Cube', 'Snoop Dogg', 'Nate Dogg', 'Daz Dillinger', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band\n",
      "3D Picnic                          1\n",
      "Benito DiPaula                     1\n",
      "DMP Big Band                       7\n",
      "DePaul University Jazz Ensemble    2\n",
      "Jacki DePiro                       3\n",
      "Mario DePriest                     1\n",
      "Nicky DePaola                      4\n",
      "Sidney DeParis                     3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "artist_check_df = lyrics_df[lyrics_df['Band'].str.contains(\"D.P.\")].groupby(['Band']).size()\n",
    "print(artist_check_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84884           Flava in Ya Ear\n",
      "84885         Funk Wit da Style\n",
      "84886                  Get Down\n",
      "84887             Judgement Day\n",
      "84888                  Mainline\n",
      "84889    Making Moves With Puff\n",
      "84890    Project: Funk da World\n",
      "84891                  Real Raw\n",
      "84892            When God Comes\n",
      "Name: Song, dtype: object\n"
     ]
    }
   ],
   "source": [
    "artist_song_check_df = lyrics_df[lyrics_df['Band'] == 'Craig Mack']['Song']\n",
    "print(artist_song_check_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (lyrics_df['Band'].isin(east_coast_artists)),\n",
    "    (lyrics_df['Band'].isin(west_coast_artists))]\n",
    "choices = ['East', 'West']\n",
    "lyrics_df['RapCoast'] = np.select(conditions, choices, default='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rap_df = lyrics_df[lyrics_df['RapCoast'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band                  RapCoast\n",
      "2Pac                  West        252\n",
      "Craig Mack            East          9\n",
      "Daz Dillinger         West         22\n",
      "Diddy                 East        104\n",
      "Dr. Dre               West         80\n",
      "Eazy-E                West         24\n",
      "Ice Cube              West        178\n",
      "N.W.A                 West         22\n",
      "Nate Dogg             West         40\n",
      "Snoop Dogg            West        344\n",
      "The Notorious B.I.G.  East         90\n",
      "Tim Dog               East          7\n",
      "Wu-Tang Clan          East        125\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(rap_df.groupby(['Band', 'RapCoast']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RapCoast\n",
      "East    335\n",
      "West    962\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(rap_df.groupby(['RapCoast']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Rap artist file to CSV : rap_artists.csv\n"
     ]
    }
   ],
   "source": [
    "# let's write this to a file\n",
    "rap_artist_filename = 'rap_artists.csv'\n",
    "rap_df.to_csv(rap_artist_filename)\n",
    "print('Wrote Rap artist file to CSV : {}'.format(rap_artist_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text_list = rap_df['Lyrics'].tolist()\n",
    "lyrics_index_list = rap_df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Now before we start working with the text, let's handle some of it's formatted.  Since many of the lyrics are back and forth between artists and sometimes parts of the songs are marked up, let's handle this by removing them so that they do not dominate our vocabulary.  Otherwise, we see the words 'snoop' and 'dogg' all over the topic model because he is featured in so many songs even if he is not the artist.  We'll do this with regular expressions:\n",
    "* Replace [ARTIST NAME] with blanks\n",
    "* Replace [chorus] with blanks\n",
    "* etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to clean strings...\n",
      " .  Who let them Eastside ridaz out?  . \n",
      "I am the verse  .  I am the part that repeats\n"
     ]
    }
   ],
   "source": [
    "test_clean_string_1 = '[Bigg Snoop Dogg] Who let them Eastside ridaz out? [Dogg] [ Dre ]'\n",
    "test_clean_string_2 = 'I am the verse [chorus] I am the part that repeats'\n",
    "\n",
    "print('About to clean strings...')\n",
    "\n",
    "# match brackets with up to 30 characters in between\n",
    "bracket_pattern = re.compile('\\[.{1,30}\\]')\n",
    "print(bracket_pattern.sub(' . ', test_clean_string_1))\n",
    "print(bracket_pattern.sub(' . ', test_clean_string_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned of formatting\n",
      "Average characters removed per document : 14.808018504240556\n"
     ]
    }
   ],
   "source": [
    "# process all of sentences for this formatting\n",
    "characters_removed_list = []\n",
    "for i in range(len(text_list)):\n",
    "    original_text = text_list[i]\n",
    "    text_length_before = len(original_text)\n",
    "    clean_format_text = bracket_pattern.sub(' . ', original_text)\n",
    "    text_length_after = len(clean_format_text)\n",
    "    characters_removed = text_length_before - text_length_after\n",
    "    characters_removed_list.append(characters_removed)\n",
    "    text_list[i] = clean_format_text\n",
    "    \n",
    "print('Text cleaned of formatting')\n",
    "print('Average characters removed per document : {}'.format(np.mean(np.array(characters_removed_list))))\n",
    "\n",
    "#print(text_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization process : [0/1297]\n",
      "Tokenization process : [1000/1297]\n",
      "Total size of tokenized list : 1297\n",
      "Total size of unique tokens : 24022\n",
      "DONE reading, tokenizing and counting\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_texts = []\n",
    "token_count_list = []\n",
    "unique_token_set = set()\n",
    "for i, text in enumerate(text_list):\n",
    "    if i % 1000 == 0:\n",
    "        print('Tokenization process : [{0}/{1}]'.format(i, len(text_list)))\n",
    "        \n",
    "    # get the index into the original text\n",
    "    index = text_list[i]\n",
    "        \n",
    "    # there are lots and lots of rows which have no lyrics at all, so let's skip them\n",
    "    if not isinstance(text, str):\n",
    "        #print('Skipping column type : {0} at index {1}'.format(type(text), index))  \n",
    "        continue\n",
    "        \n",
    "    # this is a better way to tokenize, but for the interest of time, we will tokenize with\n",
    "    # whitespace using python's split() function\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    #tokens = text.lower().split()\n",
    "    tokenized_texts.append(tokens)\n",
    "    token_count_list.append(len(tokens))\n",
    "    unique_token_set |= set(tokens)\n",
    "    \n",
    "print('Total size of tokenized list : {}'.format(len(tokenized_texts)))\n",
    "print('Total size of unique tokens : {}'.format(len(unique_token_set)))\n",
    "print('DONE reading, tokenizing and counting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Before we go much further, let's set up a dataframe to gather statistics on our vocabulary including term frequency, term rarity using Inverse Document Frequency (IDF) and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_term_stats_df(tokenized_texts, stop_word_set, enable_phrase_stats = False):\n",
    "    # TODO : Is this really the best way to calculate TF-IDF?  By document?  Shouldn't this be by vocabulary?\n",
    "    print('Building Dictionary...')\n",
    "    dictionary = gensim.corpora.Dictionary(tokenized_texts)\n",
    "    \n",
    "    print('Setting up Bag-of-Words for [{0}] documents...'.format(len(tokenized_texts)))\n",
    "    unfiltered_corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "    # now before we start using this, let's look at some term weighting with TF-IDF to see if there are some terms we can easily ignore\n",
    "    # (e.g. patient, patient, cell, cells, etc) which occur very frequently across nearly all documents\n",
    "\n",
    "    tfidf = gensim.models.TfidfModel(unfiltered_corpus, id2word=dictionary)\n",
    "    \n",
    "    print('TF-IDF model built and now setting up dataframe...')\n",
    "    global_idf_map = {}\n",
    "    term_dicts = []\n",
    "    for id in tfidf.id2word.keys():\n",
    "        word = tfidf.id2word[id]\n",
    "        global_tf = tfidf.dfs[id]\n",
    "        global_idf = tfidf.idfs[id]\n",
    "        global_tfidf = global_tf * float(global_idf)\n",
    "        global_idf_map[id] = global_tfidf\n",
    "        \n",
    "        stopword = word in stop_word_set\n",
    "        \n",
    "        any_alpha = any(c.isalpha() for c in word)\n",
    "        all_alpha = word.isalpha()\n",
    "        alpha_num = word.isalnum()\n",
    "        phrase = '_' in word\n",
    "        \n",
    "        phrase_stopword_edge = False\n",
    "        phrase_noalpha_edge = False\n",
    "        phrase_disallowed_start = False\n",
    "        if enable_phrase_stats and phrase:\n",
    "            phrase_stopword_edge = phrase_has_stopword_edge(word, stop_word_set)\n",
    "            phrase_noalpha_edge = phrase_has_no_alpha_edge(word)\n",
    "            phrase_disallowed_start =  phrase_has_disallowed_start(word)\n",
    "\n",
    "        term_dict = {'@Token' : word, 'Global TF' : global_tf, \n",
    "                     'Global TF-IDF' : global_tfidf, 'Global IDF' : global_idf,\n",
    "                    'Stopword' : stopword, 'Phrase' : phrase,\n",
    "                    'Any Alphabetic' : any_alpha, 'All Alphabetic' : all_alpha, 'All Alphanumeric' : alpha_num,\n",
    "                    #'Phrase Stopword Edge' : phrase_stopword_edge, 'Phrase No Alpha Edge' : phrase_noalpha_edge,\n",
    "                    #'Phrase Disallowed Start' : phrase_disallowed_start\n",
    "                    }\n",
    "        term_dicts.append(term_dict)\n",
    "\n",
    "    term_stats_df = pd.DataFrame(term_dicts)\n",
    "    term_stats_df = term_stats_df.sort_values('Global TF', ascending = False)\n",
    "    return term_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping stats for completely unfiltered vocabulary...\n",
      "Building Dictionary...\n",
      "Setting up Bag-of-Words for [1297] documents...\n",
      "TF-IDF model built and now setting up dataframe...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@Token</th>\n",
       "      <th>All Alphabetic</th>\n",
       "      <th>All Alphanumeric</th>\n",
       "      <th>Any Alphabetic</th>\n",
       "      <th>Global IDF</th>\n",
       "      <th>Global TF</th>\n",
       "      <th>Global TF-IDF</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6475</th>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>1296</td>\n",
       "      <td>1.442139</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21812</th>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.011166</td>\n",
       "      <td>1287</td>\n",
       "      <td>14.371190</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7697</th>\n",
       "      <td>i</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.012288</td>\n",
       "      <td>1286</td>\n",
       "      <td>15.802158</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12701</th>\n",
       "      <td>to</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.014533</td>\n",
       "      <td>1284</td>\n",
       "      <td>18.660728</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20599</th>\n",
       "      <td>and</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.022420</td>\n",
       "      <td>1277</td>\n",
       "      <td>28.630282</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      @Token  All Alphabetic  All Alphanumeric  Any Alphabetic  Global IDF  \\\n",
       "6475       .           False             False           False    0.001113   \n",
       "21812    the            True              True            True    0.011166   \n",
       "7697       i            True              True            True    0.012288   \n",
       "12701     to            True              True            True    0.014533   \n",
       "20599    and            True              True            True    0.022420   \n",
       "\n",
       "       Global TF  Global TF-IDF  Phrase  Stopword  \n",
       "6475        1296       1.442139   False     False  \n",
       "21812       1287      14.371190   False      True  \n",
       "7697        1286      15.802158   False      True  \n",
       "12701       1284      18.660728   False      True  \n",
       "20599       1277      28.630282   False      True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Prepping stats for completely unfiltered vocabulary...')\n",
    "unfiltered_term_stats_df = get_term_stats_df(tokenized_texts, stop_word_set)\n",
    "\n",
    "display(unfiltered_term_stats_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Term Stats CSV to : unfiltered_term_stats_06_30.csv\n",
      "DONE Writing Term Stats CSV to : unfiltered_term_stats_06_30.csv\n"
     ]
    }
   ],
   "source": [
    "unfiltered_term_stats_filename = 'unfiltered_term_stats_{0}.csv'.format(datetime.datetime.now().strftime(\"%m_%d\"))\n",
    "print('Writing Term Stats CSV to : {}'.format(unfiltered_term_stats_filename))\n",
    "\n",
    "unfiltered_term_stats_df.to_csv(unfiltered_term_stats_filename)\n",
    "\n",
    "print('DONE Writing Term Stats CSV to : {}'.format(unfiltered_term_stats_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the distribution of terms as well\n",
    "#global_idf_series = pd.Series(unfiltered_term_stats_df['Global IDF'], name='Global IDF')\n",
    "        \n",
    "# let's look at the distribution of TF-IDF values\n",
    "#seaborn.distplot(global_idf_series);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Now we'll process and clean the texts before we train a topic model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, token, pos):\n",
    "        self.token = token\n",
    "        self.pos = pos\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '{0}/{1}'.format(self.token, self.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CULL_ARTIST_NAMES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MIN_TERM_FREQUENCY = 3\n",
    "MIN_TOKEN_LENGTH = 3\n",
    "CULL_STOP_WORDS = True\n",
    "CULL_BY_MIN_FREQUENCY = True\n",
    "CULL_NON_ALPHA = False\n",
    "CULL_TOKENS_WITH_NO_ALPHA = True\n",
    "CULL_SHORT_TOKENS = True\n",
    "CULL_PHRASES_WITH_STOPWORD_EDGE = False\n",
    "CULL_PHRASE_NO_ALPHA_EDGE = False\n",
    "CULL_PHRASE_DISALLOWED_START = False\n",
    "CULL_NON_NOUNS = True\n",
    "FILTER_DICTIONARY_BY_GLOBAL_IDF = True\n",
    "MIN_GLOBAL_IDF_VALUE = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', 'live', 'in', 'this', 'motherfucker', 'thug', 'pound', 'bitch', '.', 'kurupt', 'daz', 'makaveli', 'idi', 'kastro', 'hussein', '.', 'you', 'know', 'bitch', 'the', 'whole', 'muthafuckin', \"'\", 'clique', '.', '.', 'we', 'gon', \"'\", 'hit', 'yo', \"'\", 'ass', 'up', '.', '.', 'you', 'know', 'where', 'we', 'comin', \"'\", 'from', '.', 'death', 'row', 'bitch', 'ay', 'yo']\n"
     ]
    }
   ],
   "source": [
    "# but for now we'll use the original text verbatim\n",
    "filtered_tokenized_texts = tokenized_texts\n",
    "\n",
    "print(filtered_tokenized_texts[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding [13] artist names to STOP WORD LIST...\n",
      "Total updates stopword set size : 206\n"
     ]
    }
   ],
   "source": [
    "if CULL_ARTIST_NAMES:\n",
    "    all_artist_name_set = set(rap_df['Band'].tolist())\n",
    "    print('Adding [{}] artist names to STOP WORD LIST...'.format(len(all_artist_name_set)))\n",
    "    for artist_name in all_artist_name_set:\n",
    "        artist_tokens = artist_name.split()\n",
    "        for artist_token in artist_tokens:\n",
    "            stop_word_set.add(artist_token.lower())\n",
    "            \n",
    "            \n",
    "    # let's also add some other variants of artist names that may be different from the names in this set\n",
    "    stop_word_set.add('pac')\n",
    "    stop_word_set.add('2-pac')\n",
    "    stop_word_set.add('2pac')\n",
    "    stop_word_set.add('biggie')\n",
    "    stop_word_set.add('smalls')\n",
    "    stop_word_set.add('smallz')\n",
    "    stop_word_set.add('dogg')\n",
    "    stop_word_set.add('doggy')\n",
    "    \n",
    "    print('Total updates stopword set size : {}'.format(len(stop_word_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Token and POS information for later culling...\n",
      "DONE with part of speech tagging\n",
      "[./., ./., live/VB, in/IN, this/DT, motherfucker/NN, thug/-None-, pound/NN, bitch/NN, ./., kurupt/JJ, daz/-None-, makaveli/-None-, idi/-None-, kastro/-None-, hussein/NNP, ./., you/PRP, know/VBP, bitch/NN, the/DT, whole/JJ, muthafuckin/NNP, '/POS, clique/NNP, ./., ./., we/PRP, gon/JJ, '/POS, hit/VBN, yo/-None-, '/POS, ass/NN, up/IN, ./., ./., you/PRP, know/VBP, where/WRB, we/PRP, comin/NNP, '/POS, from/IN, ./., death/NN, row/NN, bitch/NN, ay/-None-, yo/-None-]\n"
     ]
    }
   ],
   "source": [
    "# convert everything into this class even if we do not do POS tagging\n",
    "if CULL_NON_NOUNS:\n",
    "    print('Preparing Token and POS information for later culling...')\n",
    "    for i in range(len(filtered_tokenized_texts)):\n",
    "        document_tokens = filtered_tokenized_texts[i]\n",
    "        token_objects = [Token(pos[0], pos[1]) for pos in brill_tagger.tag(document_tokens)]\n",
    "        filtered_tokenized_texts[i] = token_objects\n",
    "        \n",
    "        #print(document_tokens)\n",
    "        #break\n",
    "    \n",
    "    print('DONE with part of speech tagging')\n",
    "    \n",
    "else:\n",
    "    print('Preparing Token information even without culling later')\n",
    "    for i in range(len(filtered_tokenized_texts)):\n",
    "        document_tokens = filtered_tokenized_texts[i]\n",
    "        token_objects = [Token(token, '') for token in document_tokens if len(token) > 0]\n",
    "        # store this back in \n",
    "        filtered_tokenized_texts[i] = token_objects\n",
    "    \n",
    "#print(filtered_tokenized_texts[:10])\n",
    "print(filtered_tokenized_texts[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culling stopwords...\n",
      "Culling by min frequency...\n",
      "Demo document tokens:\n",
      "['motherfucker', 'thug', 'pound', 'bitch', 'makaveli', 'kastro', 'hussein', 'bitch', 'muthafuckin', 'clique', 'ass', 'comin', 'death', 'row', 'bitch', 'nigga', 'westside', 'nigga', 'ballers', 'steel', 'cowards', 'panic', 'button', 'thinkin', 'wad', 'bomb', 'vietnam', 'rhyme', 'spine', 'back', 'cracker', 'smack', 'grammar', 'murder', 'alabama', 'arm', 'hammer', 'across', 'niggas', 'like', 'cause', 'niggas', 'fit', 'show', 'hoe', 'proposal', 'bitch', 'mashin', 'ford', 'hell']\n",
      "Wall time: 3.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# prepare to remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in filtered_tokenized_texts:\n",
    "    for token in text:\n",
    "        frequency[token.token] += 1\n",
    "        \n",
    "if CULL_STOP_WORDS:\n",
    "    print('Culling stopwords...')\n",
    "    # then CULL by alpha\n",
    "    filtered_tokenized_texts = [[token for token in text if token.token not in stop_word_set]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_NON_NOUNS:\n",
    "    # keep this for any noun or any phrase ('_') since phrases may not be properly labeled for part of speech\n",
    "    filtered_tokenized_texts = [[token for token in text if ('_' in token.token or get_wordnet_pos(token.pos) == nltk.corpus.wordnet.NOUN)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_PHRASES_WITH_STOPWORD_EDGE:\n",
    "    print('Culling phrases with a stopword on their EDGE')\n",
    "    filtered_tokenized_texts = [[token for token in text if not phrase_has_stopword_edge(token.token, stop_word_set)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_PHRASE_NO_ALPHA_EDGE:\n",
    "    print('Culling phrases with a NO ALPHA term on their EDGE')\n",
    "    filtered_tokenized_texts = [[token for token in text if not phrase_has_no_alpha_edge(token.token)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_PHRASE_DISALLOWED_START:\n",
    "    print('Culling phrases starting with a DISALLOWED term (e.g. \"p\", \"n\")')\n",
    "    filtered_tokenized_texts = [[token for token in text if not phrase_has_disallowed_start(token.token)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "\n",
    "if CULL_BY_MIN_FREQUENCY:\n",
    "    print('Culling by min frequency...')\n",
    "    # CULL by frequency\n",
    "    filtered_tokenized_texts = [[token for token in text if frequency[token.token] > MIN_TERM_FREQUENCY]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "# NOTE that this culling only culls tokens which have NO alpha tokens at all\n",
    "# which is very different that the processing below\n",
    "# this will enable this:\n",
    "# 'TP53' whereas the method below would cull it\n",
    "if CULL_TOKENS_WITH_NO_ALPHA:\n",
    "    filtered_tokenized_texts = [[token for token in text if any(c.isalpha() for c in token.token)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "\n",
    "if CULL_NON_ALPHA:\n",
    "    print('Culling all non-alpha tokens...')\n",
    "    # then CULL by alpha\n",
    "    filtered_tokenized_texts = [[token for token in text if token.token.isalpha()]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_SHORT_TOKENS:\n",
    "    filtered_tokenized_texts = [[token for token in text if len(token.token) >= MIN_TOKEN_LENGTH]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "# finally convert back from the object into simple token strings\n",
    "filtered_tokenized_texts = [[token.token for token in text] for text in filtered_tokenized_texts]\n",
    "\n",
    "DEMO_WORDS_TO_PRINT = 50\n",
    "\n",
    "print('Demo document tokens:')\n",
    "print(filtered_tokenized_texts[0][:DEMO_WORDS_TO_PRINT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Set up our initial dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dictionary size : [5327]\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(filtered_tokenized_texts)\n",
    "print('Total dictionary size : [{}]'.format(len(dictionary.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# But before we go on, let's see if we should filter our vocabulary of very common terms (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def filter_dictionary_by_global_idf(corpus, tokenized_texts, tfidf_model, dictionary, min_global_idf_value, \n",
    "                                    print_low_value_words = True):\n",
    "    low_value_words = set()\n",
    "    low_value_ids = set()\n",
    "    for id in tfidf.id2word.keys():\n",
    "        word = tfidf.id2word[id]\n",
    "        global_tf = tfidf.dfs[id]\n",
    "        global_idf = tfidf.idfs[id]\n",
    "        if global_idf < min_global_idf_value:\n",
    "            low_value_words.add(word)\n",
    "            low_value_ids.add(id)\n",
    "            \n",
    "    print('Total number of words removed : [{}]'.format(len(low_value_words)))\n",
    "\n",
    "    if print_low_value_words:\n",
    "        print('Here are all the words we will remove from our dictionary by global IDF')\n",
    "        print(sorted(list(low_value_words)))\n",
    "        \n",
    "        low_value_word_percent = len(low_value_words) / float(len(dictionary))\n",
    "\n",
    "        print('Total low value words : {0}'.format(len(low_value_words)))\n",
    "        print('Total low value Total Vocab percent : {0}'.format(low_value_word_percent))\n",
    "        \n",
    "    # now we can filter\n",
    "    dictionary.filter_tokens(bad_ids=low_value_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# NOTE : This class was taken from an example here:\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/dtm_example.ipynb\n",
    "\n",
    "class Corpus(gensim.corpora.textcorpus.TextCorpus):\n",
    "\n",
    "    def get_texts(self):\n",
    "        # let's also make sure that all empty documents (no tokens) are not included \n",
    "        # NOTE : The DIM model (model = 'fixed') breaks if there are any documents with no tokens\n",
    "        # also, apparently the logic for checking this is any(not text), so let's try that\n",
    "        #print('Calling a version of get_texts() that should not allow empty/None lists...')\n",
    "        return [x for x in self.input]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length : 1297\n",
      "Total dictionary size BEFORE TF-IDF filtering : [5327]\n",
      "Total number of words removed : [155]\n",
      "Here are all the words we will remove from our dictionary by global IDF\n",
      "[\"'cause\", \"'em\", \"'ll\", 'act', 'another', 'around', 'ass', 'baby', 'back', 'bang', 'bitch', 'bitches', 'black', 'block', 'blow', 'body', 'bout', 'boy', 'bust', 'call', 'cash', 'catch', 'cause', 'change', 'close', 'cold', 'comin', 'could', 'crazy', 'day', 'death', 'dick', 'dope', 'drop', 'every', 'everybody', 'everything', 'eyes', 'face', 'flow', 'fool', 'fuck', 'fuckin', 'game', 'gangsta', 'gettin', 'girl', 'god', 'guess', 'hand', 'hands', 'hate', 'head', 'heart', 'hell', 'hey', 'high', 'hit', 'hoe', 'home', 'homies', 'house', \"i'ma\", 'kick', 'life', 'like', 'livin', 'long', 'look', 'lookin', 'lot', 'low', 'mad', 'makin', 'mama', 'man', 'might', 'mind', 'mine', 'money', 'motherfucker', 'motherfuckers', 'motherfuckin', 'move', 'must', 'name', 'need', 'next', 'nigga', 'niggas', 'niggaz', 'night', 'nobody', 'nothin', 'nothing', 'one', 'pay', 'people', 'place', 'play', 'plus', 'police', 'pop', 'put', 'rap', 'remember', 'ride', 'right', 'rock', 'roll', 'rollin', 'run', 'set', 'shot', 'show', 'side', 'since', 'smoke', 'somethin', 'something', 'spot', 'start', 'step', 'still', 'straight', 'street', 'streets', 'style', 'talk', 'talkin', 'thing', 'things', 'though', 'thought', 'three', 'throw', 'thug', 'time', 'try', 'tryin', 'turn', 'two', 'wan', 'watch', 'way', 'west', 'white', 'wit', 'without', 'word', 'work', 'world', 'would', \"y'all\", 'yeah']\n",
      "Total low value words : 155\n",
      "Total low value Total Vocab percent : 0.02909705275014079\n",
      "Dictionary(5172 unique tokens: ['sniffin', 'wonda', 'fog', 'p.l.o', 'forbidden']...)\n",
      "Dictionary(5327 unique tokens: ['meter', 'sniffin', 'wonda', 'fog', 'p.l.o']...)\n",
      "Total dictionary size AFTER TF-IDF filtering : [5172]\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filtered_tokenized_texts)\n",
    "\n",
    "# this is a corpus which has not yet been pruned so we can do meaningful TF-IDF calculations with it\n",
    "# NOTE that we call \"token.token\" since it is of class token and we want the string for the actual token\n",
    "unfiltered_corpus = [dictionary.doc2bow([token.token for token in tokens]) for tokens in tokenized_texts]\n",
    "\n",
    "tfidf = gensim.models.TfidfModel(unfiltered_corpus, id2word=dictionary)\n",
    "\n",
    "print('Corpus length : {}'.format(len(corpus)))\n",
    "\n",
    "if FILTER_DICTIONARY_BY_GLOBAL_IDF:\n",
    "    print('Total dictionary size BEFORE TF-IDF filtering : [{}]'.format(len(dictionary.keys())))\n",
    "    \n",
    "    filter_dictionary_by_global_idf(unfiltered_corpus, filtered_tokenized_texts, tfidf, dictionary, MIN_GLOBAL_IDF_VALUE)\n",
    "    print(dictionary)\n",
    "    print(corpus.dictionary)\n",
    "    # re-point the new dictionary for the corpus\n",
    "    corpus.dictionary = dictionary\n",
    "    \n",
    "    print('Total dictionary size AFTER TF-IDF filtering : [{}]'.format(len(corpus.dictionary.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "NUM_TOPICS = 15\n",
    "NUM_WORKERS = 6\n",
    "VANILLA_LDA_PASSES = 5\n",
    "\n",
    "TRAIN_MULTICORE_MODEL = False\n",
    "\n",
    "# train model\n",
    "lda = None\n",
    "if TRAIN_MULTICORE_MODEL:\n",
    "    print('Training multicore model...')\n",
    "    lda = gensim.models.LdaMulticore(corpus, \n",
    "                                     id2word = dictionary, \n",
    "                                     num_topics = NUM_TOPICS, \n",
    "                                     workers = NUM_WORKERS, \n",
    "                                     passes = VANILLA_LDA_PASSES)\n",
    "else:\n",
    "    print('Training single-core model...')\n",
    "    lda = gensim.models.Lda(corpus, \n",
    "                                     id2word = dictionary, \n",
    "                                     num_topics = NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"n****\" + 0.015*\"days\" + 0.012*\"n****s\" + 0.010*\"f***\" + 0.008*\"part\" + 0.008*\"end\" + 0.007*\"vapors\" + 0.007*\"trip\" + 0.007*\"count\" + 0.007*\"gang\"'),\n",
       " (1,\n",
       "  '0.018*\"thang\" + 0.017*\"ballin\" + 0.009*\"picture\" + 0.007*\"cha\" + 0.006*\"gold\" + 0.006*\"dollar\" + 0.006*\"pimp\" + 0.005*\"nuts\" + 0.005*\"collar\" + 0.005*\"wake\"'),\n",
       " (2,\n",
       "  '0.012*\"sky\" + 0.010*\"tang\" + 0.008*\"soulja\" + 0.007*\"limit\" + 0.007*\"family\" + 0.006*\"batman\" + 0.006*\"mansion\" + 0.005*\"westside\" + 0.005*\"peace\" + 0.005*\"years\"'),\n",
       " (3,\n",
       "  '0.014*\"city\" + 0.012*\"lord\" + 0.011*\"enemies\" + 0.010*\"cop\" + 0.009*\"cali\" + 0.009*\"bye\" + 0.006*\"california\" + 0.005*\"party\" + 0.005*\"outlaw\" + 0.004*\"park\"'),\n",
       " (4,\n",
       "  '0.012*\"friends\" + 0.011*\"murder\" + 0.010*\"homie\" + 0.007*\"mourn\" + 0.007*\"doggz\" + 0.006*\"lady\" + 0.005*\"dup\" + 0.005*\"dumb\" + 0.005*\"till\" + 0.004*\"dollars\"'),\n",
       " (5,\n",
       "  '0.010*\"pac\" + 0.010*\"cha\" + 0.010*\"till\" + 0.009*\"somebody\" + 0.008*\"doggy\" + 0.007*\"loot\" + 0.007*\"funk\" + 0.006*\"gim\" + 0.006*\"mainline\" + 0.006*\"pound\"'),\n",
       " (6,\n",
       "  '0.041*\"doggy\" + 0.026*\"bomb\" + 0.013*\"do-owww-ohhhh-oggg\" + 0.012*\"rebel\" + 0.010*\"cuz\" + 0.010*\"pound\" + 0.009*\"bow\" + 0.009*\"bass\" + 0.008*\"murder\" + 0.007*\"funk\"'),\n",
       " (7,\n",
       "  '0.036*\"biggie\" + 0.016*\"gang\" + 0.010*\"haters\" + 0.010*\"bone\" + 0.006*\"yep\" + 0.006*\"music\" + 0.006*\"mutha\" + 0.006*\"trust\" + 0.005*\"hustle\" + 0.005*\"days\"'),\n",
       " (8,\n",
       "  '0.011*\"party\" + 0.010*\"n****\" + 0.009*\"platinum\" + 0.008*\"why\\'all\" + 0.007*\"s***\" + 0.007*\"inside\" + 0.007*\"hello\" + 0.007*\"pain\" + 0.006*\"ghetto\" + 0.005*\"swear\"'),\n",
       " (9,\n",
       "  '0.009*\"trust\" + 0.008*\"pray\" + 0.007*\"smokin\" + 0.007*\"gin\" + 0.006*\"trife\" + 0.006*\"juice\" + 0.006*\"jesus\" + 0.006*\"indo\" + 0.005*\"express\" + 0.005*\"feelin\"'),\n",
       " (10,\n",
       "  '0.008*\"lil\" + 0.008*\"kid\" + 0.008*\"uh-huh\" + 0.006*\"water\" + 0.006*\"fight\" + 0.006*\"runnin\" + 0.004*\"benjamins\" + 0.003*\"end\" + 0.003*\"cops\" + 0.003*\"grab\"'),\n",
       " (11,\n",
       "  '0.024*\"tha\" + 0.011*\"mouth\" + 0.009*\"kids\" + 0.008*\"boom\" + 0.007*\"boss\" + 0.005*\"til\" + 0.005*\"wonda\" + 0.005*\"beef\" + 0.004*\"im\" + 0.004*\"homie\"'),\n",
       " (12,\n",
       "  '0.011*\"wonder\" + 0.010*\"funk\" + 0.009*\"thugs\" + 0.007*\"momma\" + 0.006*\"ghetto\" + 0.006*\"peace\" + 0.005*\"war\" + 0.004*\"grab\" + 0.004*\"ooh\" + 0.003*\"nation\"'),\n",
       " (13,\n",
       "  '0.013*\"self\" + 0.013*\"club\" + 0.007*\"floor\" + 0.006*\"girls\" + 0.005*\"curse\" + 0.005*\"thank\" + 0.005*\"smoking\" + 0.005*\"problems\" + 0.004*\"swivel\" + 0.004*\"across\"'),\n",
       " (14,\n",
       "  '0.013*\"nah\" + 0.012*\"boom\" + 0.011*\"huh\" + 0.009*\"pain\" + 0.008*\"holla\" + 0.006*\"number\" + 0.005*\"\\'til\" + 0.005*\"toss\" + 0.005*\"rhymes\" + 0.005*\"weight\"')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
