{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# This notebook performs topic modeling on lyrics to that we can investigate questions including : The differences between East Coast and West Coast rap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# common Python imports\n",
    "import sys\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# these are used for NLP, Data Manipulation, etc\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some NLTK data before we get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\slick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Wall time: 996 Âµs\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\slick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Wall time: 0 ns\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\slick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time nltk.download('punkt')\n",
    "%time nltk.download('stopwords')\n",
    "%time nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And load a part-of-speech tagging model that was already trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to translate POS tags from treebank to wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tag.brill.BrillTagger object at 0x000001F11643DD30>\n",
      "[('The', 'DT'), ('cat', '-None-'), ('walked', 'VBD'), ('onto', 'IN'), ('an', 'DT'), ('airplane', 'NN')]\n",
      "['The', 'cat', 'walk', 'onto', 'an', 'airplane']\n"
     ]
    }
   ],
   "source": [
    "# set up our lemmatizer in case we enable it\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# and load a POS tagger\n",
    "# let's also load and test a Brill Part of Speech tagger which was trained on the Penn Treebank:\n",
    "BRILL_TAGGER_FILE_PATH = 'resources/treebank_brill_aubt.pickle'\n",
    "brill_tagger = pickle.load(open(BRILL_TAGGER_FILE_PATH, 'rb'))\n",
    "print(brill_tagger)\n",
    "\n",
    "# now let's kick the tires on this tagger\n",
    "test_tag_tokens = 'The cat walked onto an airplane'.split()\n",
    "print(brill_tagger.tag(test_tag_tokens))\n",
    "\n",
    "print([lemma.lemmatize(x[0], get_wordnet_pos(x[1])) for x in brill_tagger.tag(test_tag_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Get', '-None-'), ('me', 'PRP'), ('on', 'IN'), ('the', 'DT'), ('court', 'NN'), ('and', 'CC'), (\"I'm\", '-None-'), ('trouble', 'NN'), ('Last', 'JJ'), ('week', 'NN'), ('fucked', 'VBD'), ('around', 'IN'), ('and', 'CC'), ('got', 'VBD'), ('a', 'DT'), ('triple', 'RB'), ('double', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "test_sentence_2 = 'Get me on the court and I\\'m trouble Last week fucked around and got a triple double'\n",
    "print(brill_tagger.tag(test_sentence_2.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loading the dataset\n",
    "## This dataset comes from the Kaggle website at this URL: https://www.kaggle.com/artimous/every-song-you-have-heard-almost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframes from CSV.  This might take some time...\n",
      "Length of Set #1 : 250000\n",
      "Length of Set #2 : 266174\n",
      "Length of Both Set combined : 516174\n",
      "Wall time: 30.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Loading dataframes from CSV.  This might take some time...')\n",
    "\n",
    "# NOTE : Without setting the engine here, we might hit the exception : \"C error: EOF inside string ...\"\n",
    "\n",
    "# This dataset is comprised of two separate files possibly for size and download limitations\n",
    "# so we'll put them together in a moment...\n",
    "lyrics_1_df = pd.read_csv('c:/datasets/lyrics/lyrics1.csv',\n",
    "                       engine = 'python')\n",
    "lyrics_2_df = pd.read_csv('c:/datasets/lyrics/lyrics2.csv',\n",
    "                       engine = 'python')\n",
    "# now we can put them together into a single frame\n",
    "lyrics_df = pd.concat([lyrics_1_df, lyrics_2_df])\n",
    "\n",
    "print('Length of Set #1 : {}'.format(len(lyrics_1_df)))\n",
    "print('Length of Set #2 : {}'.format(len(lyrics_2_df)))\n",
    "print('Length of Both Set combined : {}'.format(len(lyrics_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#replace carriage returns with periods to see if we can split lyrics as if they are sentences\n",
    "lyrics_df = lyrics_df.replace({'\\n': ' . '}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Band</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>No, no . I ain't ever trapped out the bando . ...</td>\n",
       "      <td>Everyday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>The drinks go down and smoke goes up, I feel m...</td>\n",
       "      <td>Live Till We Die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>She don't live on planet Earth no more . She f...</td>\n",
       "      <td>The Otherside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>Trippin' off that Grigio, mobbin', lights low ...</td>\n",
       "      <td>Pinot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I see a midnight panther, so gallant and so br...</td>\n",
       "      <td>Shadows &amp; Diamonds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I just want to ready your mind . 'Cause I'll s...</td>\n",
       "      <td>Uno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elijah Harris</td>\n",
       "      <td>To believe . Or not to believe . That is the q...</td>\n",
       "      <td>Girlfriend (Main)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Elijah Levi</td>\n",
       "      <td>No one here can love or understand me . Oh, wh...</td>\n",
       "      <td>Bye Bye Blackbird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Elijah Levi</td>\n",
       "      <td>Lullaby of Birdland, that's what I  . Always h...</td>\n",
       "      <td>Lullaby of Birdland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elijah Levi</td>\n",
       "      <td>I hate to see that evening sun go down . I hat...</td>\n",
       "      <td>St. Louis Blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Band                                             Lyrics  \\\n",
       "0   Elijah Blake  No, no . I ain't ever trapped out the bando . ...   \n",
       "1   Elijah Blake  The drinks go down and smoke goes up, I feel m...   \n",
       "2   Elijah Blake  She don't live on planet Earth no more . She f...   \n",
       "3   Elijah Blake  Trippin' off that Grigio, mobbin', lights low ...   \n",
       "4   Elijah Blake  I see a midnight panther, so gallant and so br...   \n",
       "5   Elijah Blake  I just want to ready your mind . 'Cause I'll s...   \n",
       "6  Elijah Harris  To believe . Or not to believe . That is the q...   \n",
       "7    Elijah Levi  No one here can love or understand me . Oh, wh...   \n",
       "8    Elijah Levi  Lullaby of Birdland, that's what I  . Always h...   \n",
       "9    Elijah Levi  I hate to see that evening sun go down . I hat...   \n",
       "\n",
       "                  Song  \n",
       "0             Everyday  \n",
       "1     Live Till We Die  \n",
       "2        The Otherside  \n",
       "3                Pinot  \n",
       "4   Shadows & Diamonds  \n",
       "5                  Uno  \n",
       "6    Girlfriend (Main)  \n",
       "7    Bye Bye Blackbird  \n",
       "8  Lullaby of Birdland  \n",
       "9      St. Louis Blues  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Before we start do do any text analysis, let's figure out the Hip-Hop artists we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# NOTE : Could not find the following in this set : \n",
    "# EAST COAST : Nas\n",
    "# WEST COAST : Warren G, Tha Dogg Pound\n",
    "east_coast_artists = ['The Notorious B.I.G.', 'Diddy', 'Wu-Tang Clan', 'Craig Mack', 'Tim Dog']\n",
    "west_coast_artists = ['N.W.A', 'Dr. Dre', '2Pac', 'Eazy-E', 'Ice Cube', 'Snoop Dogg', 'Nate Dogg', 'Daz Dillinger', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band\n",
      "3D Picnic                          1\n",
      "Benito DiPaula                     1\n",
      "DMP Big Band                       7\n",
      "DePaul University Jazz Ensemble    2\n",
      "Jacki DePiro                       3\n",
      "Mario DePriest                     1\n",
      "Nicky DePaola                      4\n",
      "Sidney DeParis                     3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "artist_check_df = lyrics_df[lyrics_df['Band'].str.contains(\"D.P.\")].groupby(['Band']).size()\n",
    "print(artist_check_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84884           Flava in Ya Ear\n",
      "84885         Funk Wit da Style\n",
      "84886                  Get Down\n",
      "84887             Judgement Day\n",
      "84888                  Mainline\n",
      "84889    Making Moves With Puff\n",
      "84890    Project: Funk da World\n",
      "84891                  Real Raw\n",
      "84892            When God Comes\n",
      "Name: Song, dtype: object\n"
     ]
    }
   ],
   "source": [
    "artist_song_check_df = lyrics_df[lyrics_df['Band'] == 'Craig Mack']['Song']\n",
    "print(artist_song_check_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (lyrics_df['Band'].isin(east_coast_artists)),\n",
    "    (lyrics_df['Band'].isin(west_coast_artists))]\n",
    "choices = ['East', 'West']\n",
    "lyrics_df['RapCoast'] = np.select(conditions, choices, default='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rap_df = lyrics_df[lyrics_df['RapCoast'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band                  RapCoast\n",
      "2Pac                  West        252\n",
      "Craig Mack            East          9\n",
      "Daz Dillinger         West         22\n",
      "Diddy                 East        104\n",
      "Dr. Dre               West         80\n",
      "Eazy-E                West         24\n",
      "Ice Cube              West        178\n",
      "N.W.A                 West         22\n",
      "Nate Dogg             West         40\n",
      "Snoop Dogg            West        344\n",
      "The Notorious B.I.G.  East         90\n",
      "Tim Dog               East          7\n",
      "Wu-Tang Clan          East        125\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(rap_df.groupby(['Band', 'RapCoast']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RapCoast\n",
      "East    335\n",
      "West    962\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(rap_df.groupby(['RapCoast']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Rap artist file to CSV : rap_artists.csv\n"
     ]
    }
   ],
   "source": [
    "# let's write this to a file\n",
    "rap_artist_filename = 'rap_artists.csv'\n",
    "rap_df.to_csv(rap_artist_filename)\n",
    "print('Wrote Rap artist file to CSV : {}'.format(rap_artist_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text_list = rap_df['Lyrics'].tolist()\n",
    "lyrics_index_list = rap_df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization process : [0/1297]\n",
      "Total size of tokenized list : 1297\n",
      "Total size of unique tokens : 38586\n",
      "DONE reading, tokenizing and counting\n",
      "Wall time: 490 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_tokenized_list = []\n",
    "token_count_list = []\n",
    "unique_token_set = set()\n",
    "for i, text in enumerate(text_list):\n",
    "    if i % 10000 == 0:\n",
    "        print('Tokenization process : [{0}/{1}]'.format(i, len(text_list)))\n",
    "        \n",
    "    # get the index into the original text\n",
    "    index = text_list[i]\n",
    "        \n",
    "    # there are lots and lots of rows which have no lyrics at all, so let's skip them\n",
    "    if not isinstance(text, str):\n",
    "        #print('Skipping column type : {0} at index {1}'.format(type(text), index))  \n",
    "        continue\n",
    "        \n",
    "    # this is a better way to tokenize, but for the interest of time, we will tokenize with\n",
    "    # whitespace using python's split() function\n",
    "    #tokens = nltk.word_tokenize(text)\n",
    "    tokens = text.lower().split()\n",
    "    text_tokenized_list.append(tokens)\n",
    "    token_count_list.append(len(tokens))\n",
    "    unique_token_set |= set(tokens)\n",
    "    \n",
    "print('Total size of tokenized list : {}'.format(len(text_tokenized_list)))\n",
    "print('Total size of unique tokens : {}'.format(len(unique_token_set)))\n",
    "print('DONE reading, tokenizing and counting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Now we'll process and clean the texts before we train a topic model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'we', 'its', 'been', 'there', 'having', 'nor', 'be', 's', 'up', 'about', 'himself', 'have', 'by', 'of', 'no', 'should', 'ma', 'during', 'this', 'so', 'd', 're', 'couldn', 'they', 'do', 'doesn', 'such', 'y', 'whom', \"she's\", 'she', 'for', 'because', 'into', 'after', 'these', 'both', \"hasn't\", \"shan't\", 'above', 'you', \"hadn't\", 'needn', 'does', 'themselves', 'if', 'ours', 'in', 'can', 'further', 'more', 'm', 'him', 'yourself', 'ourselves', 'out', 'any', 'where', 'at', 'an', 'own', 'don', 'until', 'i', 'through', 'under', 'am', 'as', 've', 'while', 'had', 'herself', 'why', \"you've\", 'between', 'doing', 'again', \"weren't\", 'won', 'and', 'the', 'but', 'below', 'here', 'most', \"won't\", 'down', 'to', \"didn't\", 'wasn', 'which', \"couldn't\", 'what', 'did', 'when', 'now', 'are', 'weren', 'those', 'aren', 'hers', 'theirs', 'not', 'with', 'mustn', 'isn', 't', 'shan', 'few', 'his', 'o', \"should've\", \"you're\", 'some', 'hasn', \"aren't\", 'her', 'will', 'is', 'hadn', 'only', 'on', 'were', 'yourselves', 'haven', 'shouldn', 'them', 'than', 'too', 'it', 'ain', 'how', 'from', 'then', 'each', 'very', \"isn't\", \"mightn't\", 'that', 'itself', 'against', 'was', \"doesn't\", 'me', 'or', \"needn't\", 'being', 'over', \"that'll\", 'a', \"haven't\", 'your', \"it's\", \"shouldn't\", \"wouldn't\", 'my', 'didn', \"you'd\", 'their', 'mightn', 'all', 'other', 'he', 'off', 'myself', 'our', \"don't\", \"mustn't\", 'wouldn', 'same', 'once', 'has', \"wasn't\", 'before', 'll', 'just', 'yours', 'who', \"you'll\"}\n"
     ]
    }
   ],
   "source": [
    "# load a stopword set we want to use...\n",
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "stop_word_set = set(stoplist)\n",
    "print(stop_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll remove a few more from our dataset\n",
    "stop_word_set.add('get')\n",
    "stop_word_set.add('got')\n",
    "stop_word_set.add('nigga')\n",
    "stop_word_set.add('niggas')\n",
    "stop_word_set.add('bitch')\n",
    "stop_word_set.add('fuck')\n",
    "stop_word_set.add('ain\\t')\n",
    "stop_word_set.add('aint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, token, pos):\n",
    "        self.token = token\n",
    "        self.pos = pos\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '{0}/{1}'.format(self.token, self.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_TERM_FREQUENCY = 3\n",
    "MIN_TOKEN_LENGTH = 3\n",
    "CULL_STOP_WORDS = True\n",
    "CULL_BY_MIN_FREQUENCY = True\n",
    "CULL_NON_ALPHA = False\n",
    "CULL_TOKENS_WITH_NO_ALPHA = True\n",
    "CULL_SHORT_TOKENS = True\n",
    "CULL_PHRASES_WITH_STOPWORD_EDGE = False\n",
    "CULL_PHRASE_NO_ALPHA_EDGE = False\n",
    "CULL_PHRASE_DISALLOWED_START = False\n",
    "CULL_NON_NOUNS = True\n",
    "FILTER_DICTIONARY_BY_GLOBAL_IDF = False\n",
    "MIN_GLOBAL_IDF_VALUE = 4.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[pac]', '.', 'live', 'in', 'this', 'motherfucker', 'thug', 'pound', 'bitch', '.', 'kurupt', 'daz', 'makaveli', 'idi', 'kastro', 'hussein', '.', 'you', 'know', 'bitch', 'the', 'whole', \"muthafuckin'\", 'clique.', '.', 'we', \"gon'\", 'hit', \"yo'\", 'ass', 'up.', '.', 'you', 'know', 'where', 'we', \"comin'\", 'from', '.', 'death', 'row', 'bitch', 'ay', 'yo', 'kurupt', '.', 'you', 'first', 'to', 'blast']\n"
     ]
    }
   ],
   "source": [
    "# but for now we'll use the original text verbatim\n",
    "filtered_tokenized_texts = text_tokenized_list\n",
    "\n",
    "print(filtered_tokenized_texts[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Token and POS information for later culling...\n",
      "DONE with part of speech tagging\n",
      "[[pac]/-None-, ./., live/VB, in/IN, this/DT, motherfucker/NN, thug/-None-, pound/NN, bitch/NN, ./., kurupt/JJ, daz/-None-, makaveli/-None-, idi/-None-, kastro/-None-, hussein/NNP, ./., you/PRP, know/VBP, bitch/NN, the/DT, whole/JJ, muthafuckin'/NNP, clique./-None-, ./., we/PRP, gon'/-None-, hit/VBN, yo'/-None-, ass/NN, up./-None-, ./., you/PRP, know/VBP, where/WRB, we/PRP, comin'/NNP, from/IN, ./., death/NN, row/NN, bitch/NN, ay/-None-, yo/-None-, kurupt/JJ, ./., you/PRP, first/JJ, to/TO, blast/JJ]\n"
     ]
    }
   ],
   "source": [
    "# convert everything into this class even if we do not do POS tagging\n",
    "if CULL_NON_NOUNS:\n",
    "    print('Preparing Token and POS information for later culling...')\n",
    "    for i in range(len(filtered_tokenized_texts)):\n",
    "        document_tokens = filtered_tokenized_texts[i]\n",
    "        token_objects = [Token(pos[0], pos[1]) for pos in brill_tagger.tag(document_tokens)]\n",
    "        filtered_tokenized_texts[i] = token_objects\n",
    "        \n",
    "        #print(document_tokens)\n",
    "        #break\n",
    "    \n",
    "    print('DONE with part of speech tagging')\n",
    "    \n",
    "else:\n",
    "    print('Preparing Token information even without culling later')\n",
    "    for i in range(len(filtered_tokenized_texts)):\n",
    "        document_tokens = filtered_tokenized_texts[i]\n",
    "        token_objects = [Token(token, '') for token in document_tokens if len(token) > 0]\n",
    "        # store this back in \n",
    "        filtered_tokenized_texts[i] = token_objects\n",
    "    \n",
    "#print(filtered_tokenized_texts[:10])\n",
    "print(filtered_tokenized_texts[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culling stopwords...\n",
      "Culling by min frequency...\n",
      "Demo document tokens:\n",
      "['[pac]', 'motherfucker', 'thug', 'pound', 'daz', 'makaveli', 'kastro', 'hussein', \"muthafuckin'\", \"gon'\", \"yo'\", 'ass', 'up.', \"comin'\", 'death', 'row', 'westside', 'ballers', '[kurupt]', 'steel', 'cowards', 'panic', 'button', \"thinkin'\", \"shit's\", 'bomb', 'vietnam', 'rhyme', 'spine', 'back', 'cracker', 'smack', 'grammar', 'murder', 'alabama', 'arm', 'hammer', 'across', 'like', 'cause', 'fit', 'show', 'hoe', 'proposal', '(bitch)', \"mashin'\", 'ford', 'hell', 'host', 'horror']\n",
      "Wall time: 2.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# prepare to remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in filtered_tokenized_texts:\n",
    "    for token in text:\n",
    "        frequency[token.token] += 1\n",
    "        \n",
    "if CULL_STOP_WORDS:\n",
    "    print('Culling stopwords...')\n",
    "    # then CULL by alpha\n",
    "    filtered_tokenized_texts = [[token for token in text if token.token not in stop_word_set]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_NON_NOUNS:\n",
    "    # keep this for any noun or any phrase ('_') since phrases may not be properly labeled for part of speech\n",
    "    filtered_tokenized_texts = [[token for token in text if ('_' in token.token or get_wordnet_pos(token.pos) == nltk.corpus.wordnet.NOUN)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_PHRASES_WITH_STOPWORD_EDGE:\n",
    "    print('Culling phrases with a stopword on their EDGE')\n",
    "    filtered_tokenized_texts = [[token for token in text if not phrase_has_stopword_edge(token.token, stop_word_set)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_PHRASE_NO_ALPHA_EDGE:\n",
    "    print('Culling phrases with a NO ALPHA term on their EDGE')\n",
    "    filtered_tokenized_texts = [[token for token in text if not phrase_has_no_alpha_edge(token.token)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_PHRASE_DISALLOWED_START:\n",
    "    print('Culling phrases starting with a DISALLOWED term (e.g. \"p\", \"n\")')\n",
    "    filtered_tokenized_texts = [[token for token in text if not phrase_has_disallowed_start(token.token)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "\n",
    "if CULL_BY_MIN_FREQUENCY:\n",
    "    print('Culling by min frequency...')\n",
    "    # CULL by frequency\n",
    "    filtered_tokenized_texts = [[token for token in text if frequency[token.token] > MIN_TERM_FREQUENCY]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "# NOTE that this culling only culls tokens which have NO alpha tokens at all\n",
    "# which is very different that the processing below\n",
    "# this will enable this:\n",
    "# 'TP53' whereas the method below would cull it\n",
    "if CULL_TOKENS_WITH_NO_ALPHA:\n",
    "    filtered_tokenized_texts = [[token for token in text if any(c.isalpha() for c in token.token)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "\n",
    "if CULL_NON_ALPHA:\n",
    "    print('Culling all non-alpha tokens...')\n",
    "    # then CULL by alpha\n",
    "    filtered_tokenized_texts = [[token for token in text if token.token.isalpha()]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_SHORT_TOKENS:\n",
    "    filtered_tokenized_texts = [[token for token in text if len(token.token) >= MIN_TOKEN_LENGTH]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "# finally convert back from the object into simple token strings\n",
    "filtered_tokenized_texts = [[token.token for token in text] for text in filtered_tokenized_texts]\n",
    "\n",
    "DEMO_WORDS_TO_PRINT = 50\n",
    "\n",
    "print('Demo document tokens:')\n",
    "print(filtered_tokenized_texts[0][:DEMO_WORDS_TO_PRINT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dictionary size : [8185]\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(filtered_tokenized_texts)\n",
    "print('Total dictionary size : [{}]'.format(len(dictionary.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length : 1297\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in filtered_tokenized_texts]\n",
    "print('Corpus length : {}'.format(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "NUM_TOPICS = 10\n",
    "NUM_WORKERS = 6\n",
    "VANILLA_LDA_PASSES = 1\n",
    "\n",
    "# train model\n",
    "lda = gensim.models.LdaMulticore(corpus, \n",
    "                                 id2word = dictionary, \n",
    "                                 num_topics = NUM_TOPICS, \n",
    "                                 workers = NUM_WORKERS, \n",
    "                                 passes = VANILLA_LDA_PASSES)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.033*\"i\\'m\" + 0.027*\"like\" + 0.007*\"cause\" + 0.006*\"that\\'s\" + 0.005*\"man\" + 0.005*\"time\" + 0.005*\"\\'cause\" + 0.005*\"world\" + 0.005*\"ass\" + 0.005*\"money\"'),\n",
       " (1,\n",
       "  '0.039*\"i\\'m\" + 0.021*\"like\" + 0.011*\"one\" + 0.008*\"me,\" + 0.008*\"that\\'s\" + 0.007*\"cause\" + 0.007*\"wanna\" + 0.006*\"\\'cause\" + 0.005*\"it,\" + 0.005*\"y\\'all\"'),\n",
       " (2,\n",
       "  '0.034*\"i\\'m\" + 0.021*\"like\" + 0.010*\"life\" + 0.009*\"\\'cause\" + 0.008*\"cause\" + 0.007*\"time\" + 0.007*\"girl\" + 0.007*\"way\" + 0.006*\"that\\'s\" + 0.006*\"baby\"'),\n",
       " (3,\n",
       "  '0.042*\"i\\'m\" + 0.013*\"like\" + 0.008*\"that\\'s\" + 0.007*\"\\'cause\" + 0.007*\"life\" + 0.006*\"wanna\" + 0.006*\"time\" + 0.006*\"yeah\" + 0.005*\"one\" + 0.005*\"me,\"'),\n",
       " (4,\n",
       "  '0.017*\"i\\'m\" + 0.010*\"that\\'s\" + 0.009*\"wanna\" + 0.008*\"like\" + 0.007*\"ass\" + 0.006*\"me,\" + 0.006*\"time\" + 0.006*\"snoop\" + 0.005*\"niggaz\" + 0.005*\"one\"'),\n",
       " (5,\n",
       "  '0.022*\"i\\'m\" + 0.016*\"cause\" + 0.015*\"like\" + 0.009*\"one\" + 0.007*\"man\" + 0.006*\"gangsta\" + 0.006*\"ass\" + 0.006*\"life\" + 0.005*\"wanna\" + 0.005*\"nah\"'),\n",
       " (6,\n",
       "  '0.033*\"i\\'m\" + 0.017*\"like\" + 0.010*\"cause\" + 0.008*\"that\\'s\" + 0.008*\"wanna\" + 0.006*\"ass\" + 0.006*\"one\" + 0.006*\"yeah\" + 0.006*\"game\" + 0.006*\"way\"'),\n",
       " (7,\n",
       "  '0.027*\"like\" + 0.021*\"i\\'m\" + 0.010*\"one\" + 0.010*\"wanna\" + 0.008*\"cause\" + 0.006*\"that\\'s\" + 0.005*\"man\" + 0.005*\"time\" + 0.005*\"ass\" + 0.005*\"me,\"'),\n",
       " (8,\n",
       "  '0.022*\"i\\'m\" + 0.013*\"like\" + 0.009*\"man\" + 0.008*\"wanna\" + 0.008*\"that\\'s\" + 0.008*\"up,\" + 0.006*\"one\" + 0.005*\"game\" + 0.005*\"life\" + 0.005*\"baby\"'),\n",
       " (9,\n",
       "  '0.034*\"i\\'m\" + 0.016*\"like\" + 0.012*\"that\\'s\" + 0.007*\"gonna\" + 0.007*\"\\'cause\" + 0.007*\"man\" + 0.007*\"one\" + 0.007*\"gotta\" + 0.006*\"cause\" + 0.005*\"time\"')]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
