{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# This notebook performs topic modeling on lyrics to that we can investigate questions including : The differences between East Coast and West Coast rap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# common Python imports\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enable plotting in our notebook\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# these are used for NLP, Data Manipulation, etc\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some NLTK data before we get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time nltk.download('punkt')\n",
    "%time nltk.download('stopwords')\n",
    "%time nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And load a part-of-speech tagging model that was already trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to translate POS tags from treebank to wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up our lemmatizer in case we enable it\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# and load a POS tagger\n",
    "# let's also load and test a Brill Part of Speech tagger which was trained on the Penn Treebank:\n",
    "BRILL_TAGGER_FILE_PATH = 'resources/treebank_brill_aubt.pickle'\n",
    "brill_tagger = pickle.load(open(BRILL_TAGGER_FILE_PATH, 'rb'))\n",
    "print(brill_tagger)\n",
    "\n",
    "# now let's kick the tires on this tagger\n",
    "test_tag_tokens = 'The cat walked onto an airplane'.split()\n",
    "print(brill_tagger.tag(test_tag_tokens))\n",
    "\n",
    "print([lemma.lemmatize(x[0], get_wordnet_pos(x[1])) for x in brill_tagger.tag(test_tag_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_sentence_2 = 'Get me on the court and I\\'m trouble Last week fucked around and got a triple double'\n",
    "print(brill_tagger.tag(test_sentence_2.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some stopwords -- words which are commonly filtered out since they are common or do not carry much meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load a stopword set we want to use...\n",
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "stop_word_set = set(stoplist)\n",
    "\n",
    "# we'll remove a few more from our dataset\n",
    "stop_word_set.add('get')\n",
    "stop_word_set.add('got')\n",
    "stop_word_set.add('nigga')\n",
    "stop_word_set.add('niggas')\n",
    "stop_word_set.add('bitch')\n",
    "stop_word_set.add('fuck')\n",
    "stop_word_set.add('ain\\t')\n",
    "stop_word_set.add('aint')\n",
    "\n",
    "print(stop_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loading the dataset\n",
    "## This dataset comes from the Kaggle website at this URL: https://www.kaggle.com/artimous/every-song-you-have-heard-almost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Loading dataframes from CSV.  This might take some time...')\n",
    "\n",
    "# NOTE : Without setting the engine here, we might hit the exception : \"C error: EOF inside string ...\"\n",
    "\n",
    "# This dataset is comprised of two separate files possibly for size and download limitations\n",
    "# so we'll put them together in a moment...\n",
    "lyrics_1_df = pd.read_csv('c:/datasets/lyrics/lyrics1.csv',\n",
    "                       engine = 'python')\n",
    "lyrics_2_df = pd.read_csv('c:/datasets/lyrics/lyrics2.csv',\n",
    "                       engine = 'python')\n",
    "# now we can put them together into a single frame\n",
    "lyrics_df = pd.concat([lyrics_1_df, lyrics_2_df])\n",
    "\n",
    "print('Length of Set #1 : {}'.format(len(lyrics_1_df)))\n",
    "print('Length of Set #2 : {}'.format(len(lyrics_2_df)))\n",
    "print('Length of Both Set combined : {}'.format(len(lyrics_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#replace carriage returns with periods to see if we can split lyrics as if they are sentences\n",
    "lyrics_df = lyrics_df.replace({'\\n': ' . '}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lyrics_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Before we start do do any text analysis, let's figure out the Hip-Hop artists we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# NOTE : Could not find the following in this set : \n",
    "# EAST COAST : Nas\n",
    "# WEST COAST : Warren G, Tha Dogg Pound\n",
    "east_coast_artists = ['The Notorious B.I.G.', 'Diddy', 'Wu-Tang Clan', 'Craig Mack', 'Tim Dog']\n",
    "west_coast_artists = ['N.W.A', 'Dr. Dre', '2Pac', 'Eazy-E', 'Ice Cube', 'Snoop Dogg', 'Nate Dogg', 'Daz Dillinger', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "artist_check_df = lyrics_df[lyrics_df['Band'].str.contains(\"D.P.\")].groupby(['Band']).size()\n",
    "print(artist_check_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "artist_song_check_df = lyrics_df[lyrics_df['Band'] == 'Craig Mack']['Song']\n",
    "print(artist_song_check_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (lyrics_df['Band'].isin(east_coast_artists)),\n",
    "    (lyrics_df['Band'].isin(west_coast_artists))]\n",
    "choices = ['East', 'West']\n",
    "lyrics_df['RapCoast'] = np.select(conditions, choices, default='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rap_df = lyrics_df[lyrics_df['RapCoast'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(rap_df.groupby(['Band', 'RapCoast']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(rap_df.groupby(['RapCoast']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# let's write this to a file\n",
    "rap_artist_filename = 'rap_artists.csv'\n",
    "rap_df.to_csv(rap_artist_filename)\n",
    "print('Wrote Rap artist file to CSV : {}'.format(rap_artist_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text_list = rap_df['Lyrics'].tolist()\n",
    "lyrics_index_list = rap_df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now before we start working with the text, let's handle some of it's formatted.  Since many of the lyrics are back and forth between artists and sometimes parts of the songs are marked up, let's handle this by removing them so that they do not dominate our vocabulary.  Otherwise, we see the words 'snoop' and 'dogg' all over the topic model because he is featured in so many songs even if he is not the artist.  We'll do this with regular expressions:\n",
    "* Replace [ARTIST NAME] with blanks\n",
    "* Replace [chorus] with blanks\n",
    "* etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to clean strings...\n",
      " .  Who let them Eastside ridaz out?  . \n",
      "I am the verse  .  I am the part that repeats\n"
     ]
    }
   ],
   "source": [
    "test_clean_string_1 = '[Bigg Snoop Dogg] Who let them Eastside ridaz out? [Dogg] [ Dre ]'\n",
    "test_clean_string_2 = 'I am the verse [chorus] I am the part that repeats'\n",
    "\n",
    "print('About to clean strings...')\n",
    "\n",
    "# match brackets with up to 30 characters in between\n",
    "bracket_pattern = re.compile('\\[.{1,30}\\]')\n",
    "print(bracket_pattern.sub(' . ', test_clean_string_1))\n",
    "print(bracket_pattern.sub(' . ', test_clean_string_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned of formatting\n",
      "Average characters removed per document : 0.0\n"
     ]
    }
   ],
   "source": [
    "# process all of sentences for this formatting\n",
    "characters_removed_list = []\n",
    "for i in range(len(text_list)):\n",
    "    original_text = text_list[i]\n",
    "    text_length_before = len(original_text)\n",
    "    clean_format_text = bracket_pattern.sub(' . ', original_text)\n",
    "    text_length_after = len(clean_format_text)\n",
    "    characters_removed = text_length_before - text_length_after\n",
    "    characters_removed_list.append(characters_removed)\n",
    "    text_list[i] = clean_format_text\n",
    "    \n",
    "print('Text cleaned of formatting')\n",
    "print('Average characters removed per document : {}'.format(np.mean(np.array(characters_removed_list))))\n",
    "\n",
    "#print(text_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization process : [0/1297]\n",
      "Tokenization process : [1000/1297]\n",
      "Total size of tokenized list : 1297\n",
      "Total size of unique tokens : 24022\n",
      "DONE reading, tokenizing and counting\n",
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_texts = []\n",
    "token_count_list = []\n",
    "unique_token_set = set()\n",
    "for i, text in enumerate(text_list):\n",
    "    if i % 1000 == 0:\n",
    "        print('Tokenization process : [{0}/{1}]'.format(i, len(text_list)))\n",
    "        \n",
    "    # get the index into the original text\n",
    "    index = text_list[i]\n",
    "        \n",
    "    # there are lots and lots of rows which have no lyrics at all, so let's skip them\n",
    "    if not isinstance(text, str):\n",
    "        #print('Skipping column type : {0} at index {1}'.format(type(text), index))  \n",
    "        continue\n",
    "        \n",
    "    # this is a better way to tokenize, but for the interest of time, we will tokenize with\n",
    "    # whitespace using python's split() function\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    #tokens = text.lower().split()\n",
    "    tokenized_texts.append(tokens)\n",
    "    token_count_list.append(len(tokens))\n",
    "    unique_token_set |= set(tokens)\n",
    "    \n",
    "print('Total size of tokenized list : {}'.format(len(tokenized_texts)))\n",
    "print('Total size of unique tokens : {}'.format(len(unique_token_set)))\n",
    "print('DONE reading, tokenizing and counting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we go much further, let's set up a dataframe to gather statistics on our vocabulary including term frequency, term rarity using Inverse Document Frequency (IDF) and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_term_stats_df(tokenized_texts, stop_word_set, enable_phrase_stats = False):\n",
    "    # TODO : Is this really the best way to calculate TF-IDF?  By document?  Shouldn't this be by vocabulary?\n",
    "    print('Building Dictionary...')\n",
    "    dictionary = gensim.corpora.Dictionary(tokenized_texts)\n",
    "    \n",
    "    print('Setting up Bag-of-Words for [{0}] documents...'.format(len(tokenized_texts)))\n",
    "    unfiltered_corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "    # now before we start using this, let's look at some term weighting with TF-IDF to see if there are some terms we can easily ignore\n",
    "    # (e.g. patient, patient, cell, cells, etc) which occur very frequently across nearly all documents\n",
    "\n",
    "    tfidf = gensim.models.TfidfModel(unfiltered_corpus, id2word=dictionary)\n",
    "    \n",
    "    print('TF-IDF model built and now setting up dataframe...')\n",
    "    global_idf_map = {}\n",
    "    term_dicts = []\n",
    "    for id in tfidf.id2word.keys():\n",
    "        word = tfidf.id2word[id]\n",
    "        global_tf = tfidf.dfs[id]\n",
    "        global_idf = tfidf.idfs[id]\n",
    "        global_tfidf = global_tf * float(global_idf)\n",
    "        global_idf_map[id] = global_tfidf\n",
    "        \n",
    "        stopword = word in stop_word_set\n",
    "        \n",
    "        any_alpha = any(c.isalpha() for c in word)\n",
    "        all_alpha = word.isalpha()\n",
    "        alpha_num = word.isalnum()\n",
    "        phrase = '_' in word\n",
    "        \n",
    "        phrase_stopword_edge = False\n",
    "        phrase_noalpha_edge = False\n",
    "        phrase_disallowed_start = False\n",
    "        if enable_phrase_stats and phrase:\n",
    "            phrase_stopword_edge = phrase_has_stopword_edge(word, stop_word_set)\n",
    "            phrase_noalpha_edge = phrase_has_no_alpha_edge(word)\n",
    "            phrase_disallowed_start =  phrase_has_disallowed_start(word)\n",
    "\n",
    "        term_dict = {'@Token' : word, 'Global TF' : global_tf, \n",
    "                     'Global TF-IDF' : global_tfidf, 'Global IDF' : global_idf,\n",
    "                    'Stopword' : stopword, 'Phrase' : phrase,\n",
    "                    'Any Alphabetic' : any_alpha, 'All Alphabetic' : all_alpha, 'All Alphanumeric' : alpha_num,\n",
    "                    #'Phrase Stopword Edge' : phrase_stopword_edge, 'Phrase No Alpha Edge' : phrase_noalpha_edge,\n",
    "                    #'Phrase Disallowed Start' : phrase_disallowed_start\n",
    "                    }\n",
    "        term_dicts.append(term_dict)\n",
    "\n",
    "    term_stats_df = pd.DataFrame(term_dicts)\n",
    "    term_stats_df = term_stats_df.sort_values('Global TF', ascending = False)\n",
    "    return term_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping stats for completely unfiltered vocabulary...\n",
      "Building Dictionary...\n",
      "Setting up Bag-of-Words for [1297] documents...\n",
      "TF-IDF model built and now setting up dataframe...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@Token</th>\n",
       "      <th>All Alphabetic</th>\n",
       "      <th>All Alphanumeric</th>\n",
       "      <th>Any Alphabetic</th>\n",
       "      <th>Global IDF</th>\n",
       "      <th>Global TF</th>\n",
       "      <th>Global TF-IDF</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23889</th>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>1296</td>\n",
       "      <td>1.442139</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7055</th>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.011166</td>\n",
       "      <td>1287</td>\n",
       "      <td>14.371190</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21935</th>\n",
       "      <td>i</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.012288</td>\n",
       "      <td>1286</td>\n",
       "      <td>15.802158</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>to</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.014533</td>\n",
       "      <td>1284</td>\n",
       "      <td>18.660728</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21481</th>\n",
       "      <td>and</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.022420</td>\n",
       "      <td>1277</td>\n",
       "      <td>28.630282</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      @Token  All Alphabetic  All Alphanumeric  Any Alphabetic  Global IDF  \\\n",
       "23889      .           False             False           False    0.001113   \n",
       "7055     the            True              True            True    0.011166   \n",
       "21935      i            True              True            True    0.012288   \n",
       "5545      to            True              True            True    0.014533   \n",
       "21481    and            True              True            True    0.022420   \n",
       "\n",
       "       Global TF  Global TF-IDF  Phrase  Stopword  \n",
       "23889       1296       1.442139   False     False  \n",
       "7055        1287      14.371190   False      True  \n",
       "21935       1286      15.802158   False      True  \n",
       "5545        1284      18.660728   False      True  \n",
       "21481       1277      28.630282   False      True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Prepping stats for completely unfiltered vocabulary...')\n",
    "unfiltered_term_stats_df = get_term_stats_df(tokenized_texts, stop_word_set)\n",
    "\n",
    "display(unfiltered_term_stats_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Term Stats CSV to : unfiltered_term_stats_06_27.csv\n",
      "DONE Writing Term Stats CSV to : unfiltered_term_stats_06_27.csv\n"
     ]
    }
   ],
   "source": [
    "unfiltered_term_stats_filename = 'unfiltered_term_stats_{0}.csv'.format(datetime.datetime.now().strftime(\"%m_%d\"))\n",
    "print('Writing Term Stats CSV to : {}'.format(unfiltered_term_stats_filename))\n",
    "\n",
    "unfiltered_term_stats_df.to_csv(unfiltered_term_stats_filename)\n",
    "\n",
    "print('DONE Writing Term Stats CSV to : {}'.format(unfiltered_term_stats_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's plot the distribution of terms as well\n",
    "#global_idf_series = pd.Series(unfiltered_term_stats_df['Global IDF'], name='Global IDF')\n",
    "        \n",
    "# let's look at the distribution of TF-IDF values\n",
    "#seaborn.distplot(global_idf_series);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Now we'll process and clean the texts before we train a topic model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, token, pos):\n",
    "        self.token = token\n",
    "        self.pos = pos\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '{0}/{1}'.format(self.token, self.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_TERM_FREQUENCY = 3\n",
    "MIN_TOKEN_LENGTH = 3\n",
    "CULL_STOP_WORDS = True\n",
    "CULL_BY_MIN_FREQUENCY = True\n",
    "CULL_NON_ALPHA = False\n",
    "CULL_TOKENS_WITH_NO_ALPHA = True\n",
    "CULL_SHORT_TOKENS = True\n",
    "CULL_PHRASES_WITH_STOPWORD_EDGE = False\n",
    "CULL_PHRASE_NO_ALPHA_EDGE = False\n",
    "CULL_PHRASE_DISALLOWED_START = False\n",
    "CULL_NON_NOUNS = True\n",
    "FILTER_DICTIONARY_BY_GLOBAL_IDF = True\n",
    "MIN_GLOBAL_IDF_VALUE = 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '.', 'live', 'in', 'this', 'motherfucker', 'thug', 'pound', 'bitch', '.', 'kurupt', 'daz', 'makaveli', 'idi', 'kastro', 'hussein', '.', 'you', 'know', 'bitch', 'the', 'whole', 'muthafuckin', \"'\", 'clique', '.', '.', 'we', 'gon', \"'\", 'hit', 'yo', \"'\", 'ass', 'up', '.', '.', 'you', 'know', 'where', 'we', 'comin', \"'\", 'from', '.', 'death', 'row', 'bitch', 'ay', 'yo']\n"
     ]
    }
   ],
   "source": [
    "# but for now we'll use the original text verbatim\n",
    "filtered_tokenized_texts = tokenized_texts\n",
    "\n",
    "print(filtered_tokenized_texts[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Token and POS information for later culling...\n",
      "DONE with part of speech tagging\n",
      "[./., ./., live/VB, in/IN, this/DT, motherfucker/NN, thug/-None-, pound/NN, bitch/NN, ./., kurupt/JJ, daz/-None-, makaveli/-None-, idi/-None-, kastro/-None-, hussein/NNP, ./., you/PRP, know/VBP, bitch/NN, the/DT, whole/JJ, muthafuckin/NNP, '/POS, clique/NNP, ./., ./., we/PRP, gon/JJ, '/POS, hit/VBN, yo/-None-, '/POS, ass/NN, up/IN, ./., ./., you/PRP, know/VBP, where/WRB, we/PRP, comin/NNP, '/POS, from/IN, ./., death/NN, row/NN, bitch/NN, ay/-None-, yo/-None-]\n"
     ]
    }
   ],
   "source": [
    "# convert everything into this class even if we do not do POS tagging\n",
    "if CULL_NON_NOUNS:\n",
    "    print('Preparing Token and POS information for later culling...')\n",
    "    for i in range(len(filtered_tokenized_texts)):\n",
    "        document_tokens = filtered_tokenized_texts[i]\n",
    "        token_objects = [Token(pos[0], pos[1]) for pos in brill_tagger.tag(document_tokens)]\n",
    "        filtered_tokenized_texts[i] = token_objects\n",
    "        \n",
    "        #print(document_tokens)\n",
    "        #break\n",
    "    \n",
    "    print('DONE with part of speech tagging')\n",
    "    \n",
    "else:\n",
    "    print('Preparing Token information even without culling later')\n",
    "    for i in range(len(filtered_tokenized_texts)):\n",
    "        document_tokens = filtered_tokenized_texts[i]\n",
    "        token_objects = [Token(token, '') for token in document_tokens if len(token) > 0]\n",
    "        # store this back in \n",
    "        filtered_tokenized_texts[i] = token_objects\n",
    "    \n",
    "#print(filtered_tokenized_texts[:10])\n",
    "print(filtered_tokenized_texts[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culling stopwords...\n",
      "Culling by min frequency...\n",
      "Demo document tokens:\n",
      "['motherfucker', 'thug', 'pound', 'daz', 'makaveli', 'kastro', 'hussein', 'muthafuckin', 'clique', 'ass', 'comin', 'death', 'row', 'westside', 'ballers', 'steel', 'cowards', 'panic', 'button', 'thinkin', 'wad', 'bomb', 'vietnam', 'rhyme', 'spine', 'back', 'cracker', 'smack', 'grammar', 'murder', 'alabama', 'arm', 'hammer', 'across', 'like', 'cause', 'fit', 'show', 'hoe', 'proposal', 'mashin', 'ford', 'hell', 'host', 'horror', 'escape', 'box', 'somethin', 'midnight', 'glock']\n",
      "Wall time: 2.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# prepare to remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in filtered_tokenized_texts:\n",
    "    for token in text:\n",
    "        frequency[token.token] += 1\n",
    "        \n",
    "if CULL_STOP_WORDS:\n",
    "    print('Culling stopwords...')\n",
    "    # then CULL by alpha\n",
    "    filtered_tokenized_texts = [[token for token in text if token.token not in stop_word_set]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_NON_NOUNS:\n",
    "    # keep this for any noun or any phrase ('_') since phrases may not be properly labeled for part of speech\n",
    "    filtered_tokenized_texts = [[token for token in text if ('_' in token.token or get_wordnet_pos(token.pos) == nltk.corpus.wordnet.NOUN)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_PHRASES_WITH_STOPWORD_EDGE:\n",
    "    print('Culling phrases with a stopword on their EDGE')\n",
    "    filtered_tokenized_texts = [[token for token in text if not phrase_has_stopword_edge(token.token, stop_word_set)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_PHRASE_NO_ALPHA_EDGE:\n",
    "    print('Culling phrases with a NO ALPHA term on their EDGE')\n",
    "    filtered_tokenized_texts = [[token for token in text if not phrase_has_no_alpha_edge(token.token)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_PHRASE_DISALLOWED_START:\n",
    "    print('Culling phrases starting with a DISALLOWED term (e.g. \"p\", \"n\")')\n",
    "    filtered_tokenized_texts = [[token for token in text if not phrase_has_disallowed_start(token.token)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "\n",
    "if CULL_BY_MIN_FREQUENCY:\n",
    "    print('Culling by min frequency...')\n",
    "    # CULL by frequency\n",
    "    filtered_tokenized_texts = [[token for token in text if frequency[token.token] > MIN_TERM_FREQUENCY]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "# NOTE that this culling only culls tokens which have NO alpha tokens at all\n",
    "# which is very different that the processing below\n",
    "# this will enable this:\n",
    "# 'TP53' whereas the method below would cull it\n",
    "if CULL_TOKENS_WITH_NO_ALPHA:\n",
    "    filtered_tokenized_texts = [[token for token in text if any(c.isalpha() for c in token.token)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "\n",
    "if CULL_NON_ALPHA:\n",
    "    print('Culling all non-alpha tokens...')\n",
    "    # then CULL by alpha\n",
    "    filtered_tokenized_texts = [[token for token in text if token.token.isalpha()]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_SHORT_TOKENS:\n",
    "    filtered_tokenized_texts = [[token for token in text if len(token.token) >= MIN_TOKEN_LENGTH]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "# finally convert back from the object into simple token strings\n",
    "filtered_tokenized_texts = [[token.token for token in text] for text in filtered_tokenized_texts]\n",
    "\n",
    "DEMO_WORDS_TO_PRINT = 50\n",
    "\n",
    "print('Demo document tokens:')\n",
    "print(filtered_tokenized_texts[0][:DEMO_WORDS_TO_PRINT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up our initial dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dictionary size : [5342]\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(filtered_tokenized_texts)\n",
    "print('Total dictionary size : [{}]'.format(len(dictionary.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But before we go on, let's see if we should filter our vocabulary of very common terms (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_dictionary_by_global_idf(corpus, tokenized_texts, tfidf_model, dictionary, min_global_idf_value, \n",
    "                                    print_low_value_words = True):\n",
    "    low_value_words = set()\n",
    "    low_value_ids = set()\n",
    "    for id in tfidf.id2word.keys():\n",
    "        word = tfidf.id2word[id]\n",
    "        global_tf = tfidf.dfs[id]\n",
    "        global_idf = tfidf.idfs[id]\n",
    "        if global_idf < min_global_idf_value:\n",
    "            low_value_words.add(word)\n",
    "            low_value_ids.add(id)\n",
    "            \n",
    "    print('Total number of words removed : [{}]'.format(len(low_value_words)))\n",
    "\n",
    "    if print_low_value_words:\n",
    "        print('Here are all the words we will remove from our dictionary by global IDF')\n",
    "        print(sorted(list(low_value_words)))\n",
    "        \n",
    "        low_value_word_percent = len(low_value_words) / float(len(dictionary))\n",
    "\n",
    "        print('Total low value words : {0}'.format(len(low_value_words)))\n",
    "        print('Total low value Total Vocab percent : {0}'.format(low_value_word_percent))\n",
    "        \n",
    "    # now we can filter\n",
    "    dictionary.filter_tokens(bad_ids=low_value_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE : This class was taken from an example here:\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/dtm_example.ipynb\n",
    "\n",
    "class Corpus(gensim.corpora.textcorpus.TextCorpus):\n",
    "\n",
    "    def get_texts(self):\n",
    "        # let's also make sure that all empty documents (no tokens) are not included \n",
    "        # NOTE : The DIM model (model = 'fixed') breaks if there are any documents with no tokens\n",
    "        # also, apparently the logic for checking this is any(not text), so let's try that\n",
    "        #print('Calling a version of get_texts() that should not allow empty/None lists...')\n",
    "        return [x for x in self.input]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length : 1297\n",
      "Total dictionary size BEFORE TF-IDF filtering : [5342]\n",
      "Total number of words removed : [33]\n",
      "Here are all the words we will remove from our dictionary by global IDF\n",
      "[\"'cause\", \"'em\", \"'ll\", 'around', 'ass', 'baby', 'back', 'bitches', 'black', 'call', 'cause', 'could', 'day', 'every', 'game', 'hit', 'life', 'like', 'look', 'man', 'money', 'need', 'one', 'put', 'right', 'still', 'time', 'try', 'two', 'wan', 'way', \"y'all\", 'yeah']\n",
      "Total low value words : 33\n",
      "Total low value Total Vocab percent : 0.006177461624859603\n",
      "Dictionary(5309 unique tokens: ['benz-e', 'mmm', 'places', 'underground', 'score']...)\n",
      "Dictionary(5342 unique tokens: ['benz-e', 'mmm', 'places', 'underground', 'score']...)\n",
      "Total dictionary size AFTER TF-IDF filtering : [5309]\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filtered_tokenized_texts)\n",
    "\n",
    "# this is a corpus which has not yet been pruned so we can do meaningful TF-IDF calculations with it\n",
    "# NOTE that we call \"token.token\" since it is of class token and we want the string for the actual token\n",
    "unfiltered_corpus = [dictionary.doc2bow([token.token for token in tokens]) for tokens in tokenized_texts]\n",
    "\n",
    "tfidf = gensim.models.TfidfModel(unfiltered_corpus, id2word=dictionary)\n",
    "\n",
    "print('Corpus length : {}'.format(len(corpus)))\n",
    "\n",
    "if FILTER_DICTIONARY_BY_GLOBAL_IDF:\n",
    "    print('Total dictionary size BEFORE TF-IDF filtering : [{}]'.format(len(dictionary.keys())))\n",
    "    \n",
    "    filter_dictionary_by_global_idf(unfiltered_corpus, filtered_tokenized_texts, tfidf, dictionary, MIN_GLOBAL_IDF_VALUE)\n",
    "    print(dictionary)\n",
    "    print(corpus.dictionary)\n",
    "    # re-point the new dictionary for the corpus\n",
    "    corpus.dictionary = dictionary\n",
    "    \n",
    "    print('Total dictionary size AFTER TF-IDF filtering : [{}]'.format(len(corpus.dictionary.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "NUM_TOPICS = 10\n",
    "NUM_WORKERS = 6\n",
    "VANILLA_LDA_PASSES = 1\n",
    "\n",
    "# train model\n",
    "lda = gensim.models.LdaMulticore(corpus, \n",
    "                                 id2word = dictionary, \n",
    "                                 num_topics = NUM_TOPICS, \n",
    "                                 workers = NUM_WORKERS, \n",
    "                                 passes = VANILLA_LDA_PASSES)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"niggaz\" + 0.009*\"would\" + 0.008*\"gangsta\" + 0.007*\"dogg\" + 0.007*\"girl\" + 0.006*\"snoop\" + 0.005*\"crazy\" + 0.005*\"ride\" + 0.005*\"world\" + 0.005*\"another\"'),\n",
       " (1,\n",
       "  '0.008*\"niggaz\" + 0.007*\"snoop\" + 0.006*\"dogg\" + 0.006*\"god\" + 0.006*\"world\" + 0.005*\"motherfuckers\" + 0.005*\"fuckin\" + 0.005*\"side\" + 0.005*\"cali\" + 0.005*\"boom\"'),\n",
       " (2,\n",
       "  '0.009*\"thug\" + 0.008*\"dogg\" + 0.007*\"snoop\" + 0.006*\"nah\" + 0.006*\"gettin\" + 0.006*\"lord\" + 0.006*\"girl\" + 0.006*\"fuckin\" + 0.005*\"doggy\" + 0.005*\"niggaz\"'),\n",
       " (3,\n",
       "  '0.008*\"boy\" + 0.007*\"motherfucker\" + 0.006*\"fuckin\" + 0.006*\"west\" + 0.006*\"head\" + 0.005*\"thang\" + 0.005*\"crazy\" + 0.005*\"dogg\" + 0.005*\"hate\" + 0.005*\"god\"'),\n",
       " (4,\n",
       "  '0.013*\"motherfuckers\" + 0.007*\"motherfuckin\" + 0.007*\"niggaz\" + 0.007*\"mama\" + 0.006*\"ride\" + 0.005*\"world\" + 0.005*\"fuckin\" + 0.005*\"funk\" + 0.005*\"motherfucker\" + 0.004*\"gettin\"'),\n",
       " (5,\n",
       "  '0.012*\"dogg\" + 0.011*\"snoop\" + 0.008*\"body\" + 0.007*\"rock\" + 0.006*\"wit\" + 0.006*\"girl\" + 0.005*\"smoke\" + 0.005*\"dick\" + 0.005*\"gangsta\" + 0.005*\"would\"'),\n",
       " (6,\n",
       "  '0.016*\"gangsta\" + 0.010*\"girl\" + 0.007*\"boy\" + 0.007*\"gang\" + 0.005*\"ride\" + 0.005*\"motherfucker\" + 0.005*\"head\" + 0.004*\"would\" + 0.004*\"rock\" + 0.004*\"hands\"'),\n",
       " (7,\n",
       "  '0.007*\"gettin\" + 0.007*\"thug\" + 0.006*\"rebel\" + 0.006*\"streets\" + 0.005*\"thing\" + 0.005*\"world\" + 0.005*\"dogg\" + 0.005*\"tryin\" + 0.005*\"lot\" + 0.005*\"girl\"'),\n",
       " (8,\n",
       "  '0.009*\"bang\" + 0.007*\"ride\" + 0.006*\"dogg\" + 0.005*\"people\" + 0.005*\"thing\" + 0.005*\"cash\" + 0.005*\"fuckin\" + 0.005*\"motherfucker\" + 0.005*\"tha\" + 0.005*\"ballin\"'),\n",
       " (9,\n",
       "  '0.011*\"world\" + 0.007*\"wit\" + 0.007*\"hey\" + 0.005*\"home\" + 0.005*\"motherfucker\" + 0.005*\"city\" + 0.005*\"gangsta\" + 0.004*\"would\" + 0.004*\"ride\" + 0.004*\"pain\"')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
