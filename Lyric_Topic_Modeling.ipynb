{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook performs topic modeling on lyrics to that we can investigate questions including : The differences between East Coast and West Coast rap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common Python imports\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enable plotting in our notebook\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# these are used for NLP, Data Manipulation, etc\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some NLTK data before we get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\slick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Wall time: 1.3 s\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\slick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Wall time: 4.01 ms\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\slick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Wall time: 24 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time nltk.download('punkt')\n",
    "%time nltk.download('stopwords')\n",
    "%time nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And load a part-of-speech tagging model that was already trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to translate POS tags from treebank to wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that if for some reason this POS tagger .pickle file will not load, try running 'dos2unix' on it within Cygwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tag.brill.BrillTagger object at 0x00000245A21D9160>\n",
      "[('The', 'DT'), ('cat', '-None-'), ('walked', 'VBD'), ('onto', 'IN'), ('an', 'DT'), ('airplane', 'NN')]\n",
      "['The', 'cat', 'walk', 'onto', 'an', 'airplane']\n"
     ]
    }
   ],
   "source": [
    "# set up our lemmatizer in case we enable it\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# and load a POS tagger\n",
    "# let's also load and test a Brill Part of Speech tagger which was trained on the Penn Treebank:\n",
    "BRILL_TAGGER_FILE_PATH = 'resources/treebank_brill_aubt.pickle'\n",
    "brill_tagger = pickle.load(open(BRILL_TAGGER_FILE_PATH, 'rb'))\n",
    "print(brill_tagger)\n",
    "\n",
    "# now let's kick the tires on this tagger\n",
    "test_tag_tokens = 'The cat walked onto an airplane'.split()\n",
    "print(brill_tagger.tag(test_tag_tokens))\n",
    "\n",
    "print([lemma.lemmatize(x[0], get_wordnet_pos(x[1])) for x in brill_tagger.tag(test_tag_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Get', '-None-'), ('me', 'PRP'), ('on', 'IN'), ('the', 'DT'), ('court', 'NN'), ('and', 'CC'), (\"I'm\", '-None-'), ('trouble', 'NN'), ('Last', 'JJ'), ('week', 'NN'), ('fucked', 'VBD'), ('around', 'IN'), ('and', 'CC'), ('got', 'VBD'), ('a', 'DT'), ('triple', 'RB'), ('double', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "test_sentence_2 = 'Get me on the court and I\\'m trouble Last week fucked around and got a triple double'\n",
    "print(brill_tagger.tag(test_sentence_2.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some stopwords -- words which are commonly filtered out since they are common or do not carry much meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'had', 'about', 'nor', 'yourselves', 'below', 'himself', 'having', 'only', 'further', 'out', 'the', 'during', 'that', 'why', 'too', 'against', \"couldn't\", 'can', 'both', 'i', 'we', 'these', 'thee', 'than', 'of', 'wouldn', 'ourselves', \"you've\", 'before', \"haven't\", \"won't\", 'ainâ€™t', \"'bout\", 't', 've', 'in', 'he', 'hers', 'isn', 'needn', 'been', 'has', 'couldn', 'didn', 'you', 'and', 'those', \"didn't\", 'tha', \"hadn't\", 'what', 'not', 'at', 'once', 'da', 'down', 'on', 'very', \"it's\", 'will', \"should've\", \"don't\", 'do', 'few', 'this', 'shouldn', 'over', 'who', 'aint', 'being', \"you'd\", 'aren', 'it', 'y', \"weren't\", 'any', 'll', 'along', 'ain\\t', 'yours', 'her', 'between', 'gonna', 'doesn', 'then', 'its', 'yourself', 'd', 'more', 'if', 'o', 'through', \"'til\", 'or', 'there', 'him', 'where', 'most', 'no', 'such', \"isn't\", 'how', 'other', 'theirs', 'for', \"wouldn't\", 'them', 'while', 'is', 'whatz', 'wasn', \"that'll\", \"she's\", 'whom', 'themselves', 'his', 'because', \"mightn't\", 'into', 'above', \"i'mma\", 'ours', 'should', 'your', 's', 'are', 'with', 'myself', 'by', 'my', 'which', 'when', 'hadn', 'same', 'were', 'as', 'hasn', 'our', 'haven', 'was', 'until', \"doesn't\", 'dont', 'am', 'she', 'up', 'now', \"shan't\", 'don', 'be', 'ain', \"aren't\", \"you'll\", 'they', 'own', 'all', \"you're\", 'an', 'mustn', \"needn't\", 'shan', 'did', \"mustn't\", 'here', 'ma', 'so', 'after', 'mightn', 'each', 'beyond', 'does', 'some', 'off', 'again', 'have', \"wasn't\", 'but', 'herself', 'to', \"hasn't\", \"shouldn't\", 'doing', 'weren', 'from', 'just', 'their', 'iâ€™m', 'won', 'under', 're', 'itself', 'me', 'a', 'm'}\n"
     ]
    }
   ],
   "source": [
    "# load a stopword set we want to use...\n",
    "stoplist = nltk.corpus.stopwords.words('english')\n",
    "stop_word_set = set(stoplist)\n",
    "\n",
    "# variant spellings of stop words\n",
    "stop_word_set.add('tha')\n",
    "stop_word_set.add('da')\n",
    "stop_word_set.add('ain\\t')\n",
    "stop_word_set.add('aint')\n",
    "stop_word_set.add('gonna')\n",
    "stop_word_set.add('\\'bout')\n",
    "stop_word_set.add('\\'til')\n",
    "stop_word_set.add('ainâ€™t')\n",
    "stop_word_set.add('iâ€™m')\n",
    "stop_word_set.add('i\\'mma')\n",
    "stop_word_set.add('thee')\n",
    "stop_word_set.add('whatz')\n",
    "stop_word_set.add('along')\n",
    "stop_word_set.add('dont')\n",
    "stop_word_set.add('beyond')\n",
    "\n",
    "# we'll remove a few more from our dataset\n",
    "if False:\n",
    "    stop_word_set.add('get')\n",
    "    stop_word_set.add('got')\n",
    "    stop_word_set.add('nigga')\n",
    "    stop_word_set.add('niggas')\n",
    "    stop_word_set.add('bitch')\n",
    "    stop_word_set.add('fuck')\n",
    "    stop_word_set.add('ain\\t')\n",
    "    stop_word_set.add('aint')\n",
    "\n",
    "print(stop_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset\n",
    "## This dataset comes from the Kaggle website at this URL: https://www.kaggle.com/artimous/every-song-you-have-heard-almost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframes from CSV.  This might take some time...\n",
      "Length of Set #1 : 250000\n",
      "Length of Set #2 : 266174\n",
      "Length of Both Set combined : 516174\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Loading dataframes from CSV.  This might take some time...')\n",
    "\n",
    "# NOTE : Without setting the engine here, we might hit the exception : \"C error: EOF inside string ...\"\n",
    "\n",
    "# This dataset is comprised of two separate files possibly for size and download limitations\n",
    "# so we'll put them together in a moment...\n",
    "lyrics_1_df = pd.read_csv('c:/datasets/lyrics/lyrics1.csv',\n",
    "                       engine = 'python')\n",
    "lyrics_2_df = pd.read_csv('c:/datasets/lyrics/lyrics2.csv',\n",
    "                       engine = 'python')\n",
    "# now we can put them together into a single frame\n",
    "lyrics_df = pd.concat([lyrics_1_df, lyrics_2_df])\n",
    "\n",
    "print('Length of Set #1 : {}'.format(len(lyrics_1_df)))\n",
    "print('Length of Set #2 : {}'.format(len(lyrics_2_df)))\n",
    "print('Length of Both Set combined : {}'.format(len(lyrics_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace carriage returns with periods to see if we can split lyrics as if they are sentences\n",
    "lyrics_df = lyrics_df.replace({'\\n': ' . '}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Band</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>No, no . I ain't ever trapped out the bando . ...</td>\n",
       "      <td>Everyday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>The drinks go down and smoke goes up, I feel m...</td>\n",
       "      <td>Live Till We Die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>She don't live on planet Earth no more . She f...</td>\n",
       "      <td>The Otherside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>Trippin' off that Grigio, mobbin', lights low ...</td>\n",
       "      <td>Pinot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I see a midnight panther, so gallant and so br...</td>\n",
       "      <td>Shadows &amp; Diamonds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I just want to ready your mind . 'Cause I'll s...</td>\n",
       "      <td>Uno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elijah Harris</td>\n",
       "      <td>To believe . Or not to believe . That is the q...</td>\n",
       "      <td>Girlfriend (Main)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Elijah Levi</td>\n",
       "      <td>No one here can love or understand me . Oh, wh...</td>\n",
       "      <td>Bye Bye Blackbird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Elijah Levi</td>\n",
       "      <td>Lullaby of Birdland, that's what I  . Always h...</td>\n",
       "      <td>Lullaby of Birdland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elijah Levi</td>\n",
       "      <td>I hate to see that evening sun go down . I hat...</td>\n",
       "      <td>St. Louis Blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Band                                             Lyrics  \\\n",
       "0   Elijah Blake  No, no . I ain't ever trapped out the bando . ...   \n",
       "1   Elijah Blake  The drinks go down and smoke goes up, I feel m...   \n",
       "2   Elijah Blake  She don't live on planet Earth no more . She f...   \n",
       "3   Elijah Blake  Trippin' off that Grigio, mobbin', lights low ...   \n",
       "4   Elijah Blake  I see a midnight panther, so gallant and so br...   \n",
       "5   Elijah Blake  I just want to ready your mind . 'Cause I'll s...   \n",
       "6  Elijah Harris  To believe . Or not to believe . That is the q...   \n",
       "7    Elijah Levi  No one here can love or understand me . Oh, wh...   \n",
       "8    Elijah Levi  Lullaby of Birdland, that's what I  . Always h...   \n",
       "9    Elijah Levi  I hate to see that evening sun go down . I hat...   \n",
       "\n",
       "                  Song  \n",
       "0             Everyday  \n",
       "1     Live Till We Die  \n",
       "2        The Otherside  \n",
       "3                Pinot  \n",
       "4   Shadows & Diamonds  \n",
       "5                  Uno  \n",
       "6    Girlfriend (Main)  \n",
       "7    Bye Bye Blackbird  \n",
       "8  Lullaby of Birdland  \n",
       "9      St. Louis Blues  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we start do do any text analysis, let's figure out the Hip-Hop artists we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE : Could not find the following in this set : \n",
    "# EAST COAST : Nas\n",
    "# WEST COAST : Warren G, Tha Dogg Pound\n",
    "east_coast_artists = ['The Notorious B.I.G.', 'Diddy', 'Wu-Tang Clan', 'Craig Mack', 'Tim Dog', \n",
    "                      'Queen Latifah', 'LL Cool J', 'Da Brat', 'Missy Elliott', 'MC Lyte', 'De La Soul', 'Busta Rhymes', 'Mase', 'Q-Tip']\n",
    "west_coast_artists = ['N.W.A', 'Dr. Dre', '2Pac', 'Eazy-E', 'Ice Cube', 'Snoop Dogg', 'Nate Dogg', 'Daz Dillinger', 'Coolio']\n",
    "\n",
    "male_artists = ['N.W.A', 'Dr. Dre', '2Pac', 'Eazy-E', 'Ice Cube', 'Snoop Dogg', 'Nate Dogg', 'Daz Dillinger', \n",
    "                    'The Notorious B.I.G.', 'Diddy', 'Wu-Tang Clan', 'Craig Mack', 'Tim Dog', \n",
    "                      'LL Cool J', 'De La Soul', 'Busta Rhymes', 'Mase', 'Q-Tip', 'Coolio']\n",
    "female_artists = ['Queen Latifah', 'Da Brat', 'Missy Elliott', 'MC Lyte']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band\n",
      "Chicory Tip                      6\n",
      "George Tipton                    1\n",
      "Glenn Tipton                    13\n",
      "Hollow Tip                       3\n",
      "Julie Tippetts                   6\n",
      "La Tipica Orquestra de Tango     1\n",
      "Q-Tip                           29\n",
      "Q-Tips                           3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "artist_check_df = lyrics_df[lyrics_df['Band'].str.contains(\"Tip\")].groupby(['Band']).size()\n",
    "print(artist_check_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84884           Flava in Ya Ear\n",
      "84885         Funk Wit da Style\n",
      "84886                  Get Down\n",
      "84887             Judgement Day\n",
      "84888                  Mainline\n",
      "84889    Making Moves With Puff\n",
      "84890    Project: Funk da World\n",
      "84891                  Real Raw\n",
      "84892            When God Comes\n",
      "Name: Song, dtype: object\n"
     ]
    }
   ],
   "source": [
    "artist_song_check_df = lyrics_df[lyrics_df['Band'] == 'Craig Mack']['Song']\n",
    "print(artist_song_check_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coast_conditions = [\n",
    "    (lyrics_df['Band'].isin(east_coast_artists)),\n",
    "    (lyrics_df['Band'].isin(west_coast_artists))]\n",
    "coast_choices = ['East', 'West']\n",
    "lyrics_df['Coast'] = np.select(coast_conditions, coast_choices, default='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gender_conditions = [\n",
    "    (lyrics_df['Band'].isin(male_artists)),\n",
    "    (lyrics_df['Band'].isin(female_artists))]\n",
    "gender_choices = ['Male', 'Female']\n",
    "lyrics_df['Gender'] = np.select(gender_conditions, gender_choices, default='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rap_df = lyrics_df[lyrics_df['Coast'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band                  Coast  Gender\n",
      "2Pac                  West   Male      252\n",
      "Busta Rhymes          East   Male      159\n",
      "Coolio                West   Male       66\n",
      "Craig Mack            East   Male        9\n",
      "Da Brat               East   Female     46\n",
      "Daz Dillinger         West   Male       22\n",
      "De La Soul            East   Male      132\n",
      "Diddy                 East   Male      104\n",
      "Dr. Dre               West   Male       80\n",
      "Eazy-E                West   Male       24\n",
      "Ice Cube              West   Male      178\n",
      "LL Cool J             East   Male      167\n",
      "MC Lyte               East   Female     59\n",
      "Mase                  East   Male       53\n",
      "Missy Elliott         East   Female    127\n",
      "N.W.A                 West   Male       22\n",
      "Nate Dogg             West   Male       40\n",
      "Q-Tip                 East   Male       29\n",
      "Queen Latifah         East   Female     50\n",
      "Snoop Dogg            West   Male      344\n",
      "The Notorious B.I.G.  East   Male       90\n",
      "Tim Dog               East   Male        7\n",
      "Wu-Tang Clan          East   Male      125\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(rap_df.groupby(['Band', 'Coast', 'Gender']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coast\n",
      "East    1157\n",
      "West    1028\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(rap_df.groupby(['Coast']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender\n",
      "Female     282\n",
      "Male      1903\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(rap_df.groupby(['Gender']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Rap artist file to CSV : rap_artists.csv\n"
     ]
    }
   ],
   "source": [
    "# let's write this to a file\n",
    "rap_artist_filename = 'rap_artists.csv'\n",
    "rap_df.to_csv(rap_artist_filename)\n",
    "print('Wrote Rap artist file to CSV : {}'.format(rap_artist_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_list = rap_df['Lyrics'].tolist()\n",
    "index_list = rap_df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now before we start working with the text, let's handle some of it's formatted.  Since many of the lyrics are back and forth between artists and sometimes parts of the songs are marked up, let's handle this by removing them so that they do not dominate our vocabulary.  Otherwise, we see the words 'snoop' and 'dogg' all over the topic model because he is featured in so many songs even if he is not the artist.  We'll do this with regular expressions:\n",
    "* Replace [ARTIST NAME] with blanks\n",
    "* Replace [chorus] with blanks\n",
    "* etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to clean strings...\n",
      " .  Who let them Eastside ridaz out?  . \n",
      "I am the verse  .  I am the part that repeats\n"
     ]
    }
   ],
   "source": [
    "test_clean_string_1 = '[Bigg Snoop Dogg] Who let them Eastside ridaz out? [Dogg] [ Dre ]'\n",
    "test_clean_string_2 = 'I am the verse [chorus] I am the part that repeats'\n",
    "\n",
    "print('About to clean strings...')\n",
    "\n",
    "# match brackets with up to 30 characters in between\n",
    "bracket_pattern = re.compile('\\[.{1,30}\\]')\n",
    "print(bracket_pattern.sub(' . ', test_clean_string_1))\n",
    "print(bracket_pattern.sub(' . ', test_clean_string_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaned of formatting\n",
      "Average characters removed per document : 16.184439359267735\n"
     ]
    }
   ],
   "source": [
    "# process all of sentences for this formatting\n",
    "characters_removed_list = []\n",
    "for i in range(len(text_list)):\n",
    "    original_text = text_list[i]\n",
    "    text_length_before = len(original_text)\n",
    "    clean_format_text = bracket_pattern.sub(' . ', original_text)\n",
    "    text_length_after = len(clean_format_text)\n",
    "    characters_removed = text_length_before - text_length_after\n",
    "    characters_removed_list.append(characters_removed)\n",
    "    text_list[i] = clean_format_text\n",
    "    \n",
    "print('Text cleaned of formatting')\n",
    "print('Average characters removed per document : {}'.format(np.mean(np.array(characters_removed_list))))\n",
    "\n",
    "#print(text_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization process : [0/2185]\n",
      "Tokenization process : [1000/2185]\n",
      "Tokenization process : [2000/2185]\n",
      "Total size of tokenized list : 2185\n",
      "Total size of unique tokens : 33120\n",
      "DONE reading, tokenizing and counting\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_texts = []\n",
    "token_count_list = []\n",
    "unique_token_set = set()\n",
    "for i, text in enumerate(text_list):\n",
    "    if i % 1000 == 0:\n",
    "        print('Tokenization process : [{0}/{1}]'.format(i, len(text_list)))\n",
    "        \n",
    "    # get the index into the original text\n",
    "    index = text_list[i]\n",
    "        \n",
    "    # there are lots and lots of rows which have no lyrics at all, so let's skip them\n",
    "    if not isinstance(text, str):\n",
    "        #print('Skipping column type : {0} at index {1}'.format(type(text), index))  \n",
    "        continue\n",
    "        \n",
    "    # this is a better way to tokenize, but for the interest of time, we will tokenize with\n",
    "    # whitespace using python's split() function\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    #tokens = text.lower().split()\n",
    "    tokenized_texts.append(tokens)\n",
    "    token_count_list.append(len(tokens))\n",
    "    unique_token_set |= set(tokens)\n",
    "    \n",
    "print('Total size of tokenized list : {}'.format(len(tokenized_texts)))\n",
    "print('Total size of unique tokens : {}'.format(len(unique_token_set)))\n",
    "print('DONE reading, tokenizing and counting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we go much further, let's set up a dataframe to gather statistics on our vocabulary including term frequency, term rarity using Inverse Document Frequency (IDF) and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_term_stats_df(tokenized_texts, stop_word_set, enable_phrase_stats = False):\n",
    "    # TODO : Is this really the best way to calculate TF-IDF?  By document?  Shouldn't this be by vocabulary?\n",
    "    print('Building Dictionary...')\n",
    "    dictionary = gensim.corpora.Dictionary(tokenized_texts)\n",
    "    \n",
    "    print('Setting up Bag-of-Words for [{0}] documents...'.format(len(tokenized_texts)))\n",
    "    unfiltered_corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "    # now before we start using this, let's look at some term weighting with TF-IDF to see if there are some terms we can easily ignore\n",
    "    # (e.g. patient, patient, cell, cells, etc) which occur very frequently across nearly all documents\n",
    "\n",
    "    tfidf = gensim.models.TfidfModel(unfiltered_corpus, id2word=dictionary)\n",
    "    \n",
    "    print('TF-IDF model built and now setting up dataframe...')\n",
    "    global_idf_map = {}\n",
    "    term_dicts = []\n",
    "    for id in tfidf.id2word.keys():\n",
    "        word = tfidf.id2word[id]\n",
    "        global_tf = tfidf.dfs[id]\n",
    "        global_idf = tfidf.idfs[id]\n",
    "        global_tfidf = global_tf * float(global_idf)\n",
    "        global_idf_map[id] = global_tfidf\n",
    "        \n",
    "        stopword = word in stop_word_set\n",
    "        \n",
    "        any_alpha = any(c.isalpha() for c in word)\n",
    "        all_alpha = word.isalpha()\n",
    "        alpha_num = word.isalnum()\n",
    "        phrase = '_' in word\n",
    "        \n",
    "        phrase_stopword_edge = False\n",
    "        phrase_noalpha_edge = False\n",
    "        phrase_disallowed_start = False\n",
    "        if enable_phrase_stats and phrase:\n",
    "            phrase_stopword_edge = phrase_has_stopword_edge(word, stop_word_set)\n",
    "            phrase_noalpha_edge = phrase_has_no_alpha_edge(word)\n",
    "            phrase_disallowed_start =  phrase_has_disallowed_start(word)\n",
    "\n",
    "        term_dict = {'@Token' : word, 'Global TF' : global_tf, \n",
    "                     'Global TF-IDF' : global_tfidf, 'Global IDF' : global_idf,\n",
    "                    'Stopword' : stopword, 'Phrase' : phrase,\n",
    "                    'Any Alphabetic' : any_alpha, 'All Alphabetic' : all_alpha, 'All Alphanumeric' : alpha_num,\n",
    "                    #'Phrase Stopword Edge' : phrase_stopword_edge, 'Phrase No Alpha Edge' : phrase_noalpha_edge,\n",
    "                    #'Phrase Disallowed Start' : phrase_disallowed_start\n",
    "                    }\n",
    "        term_dicts.append(term_dict)\n",
    "\n",
    "    term_stats_df = pd.DataFrame(term_dicts)\n",
    "    term_stats_df = term_stats_df.sort_values('Global TF', ascending = False)\n",
    "    return term_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping stats for completely unfiltered vocabulary...\n",
      "Building Dictionary...\n",
      "Setting up Bag-of-Words for [2185] documents...\n",
      "TF-IDF model built and now setting up dataframe...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@Token</th>\n",
       "      <th>All Alphabetic</th>\n",
       "      <th>All Alphanumeric</th>\n",
       "      <th>Any Alphabetic</th>\n",
       "      <th>Global IDF</th>\n",
       "      <th>Global TF</th>\n",
       "      <th>Global TF-IDF</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>2184</td>\n",
       "      <td>1.442365</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>the</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>2166</td>\n",
       "      <td>27.291680</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>i</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.019945</td>\n",
       "      <td>2155</td>\n",
       "      <td>42.982359</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>to</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>2152</td>\n",
       "      <td>47.247594</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>and</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.028001</td>\n",
       "      <td>2143</td>\n",
       "      <td>60.007064</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    @Token  All Alphabetic  All Alphanumeric  Any Alphabetic  Global IDF  \\\n",
       "9        .           False             False           False    0.000660   \n",
       "237    the            True              True            True    0.012600   \n",
       "124      i            True              True            True    0.019945   \n",
       "240     to            True              True            True    0.021955   \n",
       "28     and            True              True            True    0.028001   \n",
       "\n",
       "     Global TF  Global TF-IDF  Phrase  Stopword  \n",
       "9         2184       1.442365   False     False  \n",
       "237       2166      27.291680   False      True  \n",
       "124       2155      42.982359   False      True  \n",
       "240       2152      47.247594   False      True  \n",
       "28        2143      60.007064   False      True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Prepping stats for completely unfiltered vocabulary...')\n",
    "unfiltered_term_stats_df = get_term_stats_df(tokenized_texts, stop_word_set)\n",
    "\n",
    "display(unfiltered_term_stats_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Term Stats CSV to : unfiltered_term_stats_07_04.csv\n",
      "DONE Writing Term Stats CSV to : unfiltered_term_stats_07_04.csv\n"
     ]
    }
   ],
   "source": [
    "unfiltered_term_stats_filename = 'unfiltered_term_stats_{0}.csv'.format(datetime.datetime.now().strftime(\"%m_%d\"))\n",
    "print('Writing Term Stats CSV to : {}'.format(unfiltered_term_stats_filename))\n",
    "\n",
    "unfiltered_term_stats_df.to_csv(unfiltered_term_stats_filename)\n",
    "\n",
    "print('DONE Writing Term Stats CSV to : {}'.format(unfiltered_term_stats_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the distribution of terms as well\n",
    "#global_idf_series = pd.Series(unfiltered_term_stats_df['Global IDF'], name='Global IDF')\n",
    "        \n",
    "# let's look at the distribution of TF-IDF values\n",
    "#seaborn.distplot(global_idf_series);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now we'll process and clean the texts before we train a topic model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, token, pos):\n",
    "        self.token = token\n",
    "        self.pos = pos\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '{0}/{1}'.format(self.token, self.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CULL_ARTIST_NAMES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_TERM_FREQUENCY = 10\n",
    "MIN_TOKEN_LENGTH = 4\n",
    "CULL_STOP_WORDS = True\n",
    "CULL_BY_MIN_FREQUENCY = True\n",
    "CULL_NON_ALPHA = True\n",
    "CULL_TOKENS_WITH_NO_ALPHA = True\n",
    "CULL_SHORT_TOKENS = True\n",
    "CULL_SLANG_IN_VERBS = True\n",
    "CULL_PHRASES_WITH_STOPWORD_EDGE = False\n",
    "CULL_PHRASE_NO_ALPHA_EDGE = False\n",
    "CULL_PHRASE_DISALLOWED_START = False\n",
    "CULL_NON_NOUNS = True\n",
    "FILTER_DICTIONARY_BY_GLOBAL_IDF = True\n",
    "MIN_GLOBAL_IDF_VALUE = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['his', 'realm', 'is', 'a', 'new', 'horizon', 'of', 'cuts', '.', 'they', 'expand', 'from', 'his', 'hand', 'as', 'he', 'conducts', '.', 'aim', 'a', 'lot', 'of', 'cool', 'projects', 'not', 'photography', '.', 'cut-creator', ',', 'philly-phil', \"'s\", 'biography', '.', '.', 'the', 'lyrical', 'virtual', 'also', 'makes', 'his', 'return', '.', 'all', 'adversaries', 'should', 'be', 'concerned', '.', 'll', 'cool']\n"
     ]
    }
   ],
   "source": [
    "# but for now we'll use the original text verbatim\n",
    "filtered_tokenized_texts = tokenized_texts\n",
    "\n",
    "print(filtered_tokenized_texts[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding [23] artist names to STOP WORD LIST...\n",
      "Total updates stopword set size : 240\n"
     ]
    }
   ],
   "source": [
    "if CULL_ARTIST_NAMES:\n",
    "    all_artist_name_set = set(rap_df['Band'].tolist())\n",
    "    print('Adding [{}] artist names to STOP WORD LIST...'.format(len(all_artist_name_set)))\n",
    "    for artist_name in all_artist_name_set:\n",
    "        artist_tokens = artist_name.split()\n",
    "        for artist_token in artist_tokens:\n",
    "            stop_word_set.add(artist_token.lower())\n",
    "            \n",
    "            \n",
    "    # let's also add some other variants of artist names that may be different from the names in this set\n",
    "    stop_word_set.add('pac')\n",
    "    stop_word_set.add('2-pac')\n",
    "    stop_word_set.add('2pac')\n",
    "    stop_word_set.add('biggie')\n",
    "    stop_word_set.add('smalls')\n",
    "    stop_word_set.add('smallz')\n",
    "    stop_word_set.add('dogg')\n",
    "    stop_word_set.add('doggy')\n",
    "    stop_word_set.add('bone')\n",
    "    stop_word_set.add('tang')\n",
    "    \n",
    "    print('Total updates stopword set size : {}'.format(len(stop_word_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Token and POS information for later culling...\n",
      "DONE with part of speech tagging\n",
      "[his/PRP$, realm/-None-, is/VBZ, a/DT, new/JJ, horizon/NNP, of/IN, cuts/NNS, ./., they/PRP, expand/VB, from/IN, his/PRP$, hand/NN, as/IN, he/PRP, conducts/NNS, ./., aim/VB, a/DT, lot/NN, of/IN, cool/JJ, projects/NNS, not/RB, photography/NN, ./., cut-creator/NN, ,/,, philly-phil/-None-, 's/POS, biography/NN, ./., ./., the/DT, lyrical/JJ, virtual/JJ, also/RB, makes/VBZ, his/PRP$, return/NN, ./., all/DT, adversaries/NNS, should/MD, be/VB, concerned/VBN, ./., ll/-None-, cool/JJ]\n"
     ]
    }
   ],
   "source": [
    "# convert everything into this class even if we do not do POS tagging\n",
    "if CULL_NON_NOUNS:\n",
    "    print('Preparing Token and POS information for later culling...')\n",
    "    for i in range(len(filtered_tokenized_texts)):\n",
    "        document_tokens = filtered_tokenized_texts[i]\n",
    "        token_objects = [Token(pos[0], pos[1]) for pos in brill_tagger.tag(document_tokens)]\n",
    "        filtered_tokenized_texts[i] = token_objects\n",
    "        \n",
    "        #print(document_tokens)\n",
    "        #break\n",
    "    \n",
    "    print('DONE with part of speech tagging')\n",
    "    \n",
    "else:\n",
    "    print('Preparing Token information even without culling later')\n",
    "    for i in range(len(filtered_tokenized_texts)):\n",
    "        document_tokens = filtered_tokenized_texts[i]\n",
    "        token_objects = [Token(token, '') for token in document_tokens if len(token) > 0]\n",
    "        # store this back in \n",
    "        filtered_tokenized_texts[i] = token_objects\n",
    "    \n",
    "#print(filtered_tokenized_texts[:10])\n",
    "print(filtered_tokenized_texts[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culling by min frequency...\n",
      "Culling stopwords...\n",
      "Culling all non-alpha tokens...\n",
      "Culling terms ending with 'in...\n",
      "Demo document tokens:\n",
      "['cuts', 'hand', 'projects', 'return', 'adversaries', 'friend', 'till', 'like', 'head', 'decision', 'eggs', 'lyrics', 'subject', 'matter', 'sucker', 'scatter', 'techniques', 'cuts', 'name', 'boys', 'could', 'brains', 'change', 'tree', 'yeah', 'stereo', 'zero', 'rappers', 'talent', 'mine', 'cuts', 'rhyme', 'bass', 'every', 'sucker', 'place', 'phrase', 'record', 'scratch', 'except', 'hell', 'cuts', 'like', 'blade', 'squeeze', 'record', 'bars', 'record', 'boulevard', 'rest']\n",
      "Wall time: 2.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Finally, prepare to remove words below a frequency threshold\n",
    "frequency = defaultdict(int)\n",
    "for text in filtered_tokenized_texts:\n",
    "    for token in text:\n",
    "        frequency[token.token] += 1\n",
    "\n",
    "if CULL_BY_MIN_FREQUENCY:\n",
    "    print('Culling by min frequency...')\n",
    "    # CULL by frequency\n",
    "    filtered_tokenized_texts = [[token for token in text if frequency[token.token] > MIN_TERM_FREQUENCY]\n",
    "             for text in filtered_tokenized_texts]\n",
    "        \n",
    "if CULL_STOP_WORDS:\n",
    "    print('Culling stopwords...')\n",
    "    # then CULL by alpha\n",
    "    filtered_tokenized_texts = [[token for token in text if token.token not in stop_word_set]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_NON_NOUNS:\n",
    "    # keep this for any noun or any phrase ('_') since phrases may not be properly labeled for part of speech\n",
    "    filtered_tokenized_texts = [[token for token in text if ('_' in token.token or get_wordnet_pos(token.pos) == nltk.corpus.wordnet.NOUN)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_PHRASES_WITH_STOPWORD_EDGE:\n",
    "    print('Culling phrases with a stopword on their EDGE')\n",
    "    filtered_tokenized_texts = [[token for token in text if not phrase_has_stopword_edge(token.token, stop_word_set)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_PHRASE_NO_ALPHA_EDGE:\n",
    "    print('Culling phrases with a NO ALPHA term on their EDGE')\n",
    "    filtered_tokenized_texts = [[token for token in text if not phrase_has_no_alpha_edge(token.token)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_PHRASE_DISALLOWED_START:\n",
    "    print('Culling phrases starting with a DISALLOWED term (e.g. \"p\", \"n\")')\n",
    "    filtered_tokenized_texts = [[token for token in text if not phrase_has_disallowed_start(token.token)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "# NOTE that this culling only culls tokens which have NO alpha tokens at all\n",
    "# which is very different that the processing below\n",
    "# this will enable this:\n",
    "# 'TP53' whereas the method below would cull it\n",
    "if CULL_TOKENS_WITH_NO_ALPHA:\n",
    "    filtered_tokenized_texts = [[token for token in text if any(c.isalpha() for c in token.token)]\n",
    "             for text in filtered_tokenized_texts]\n",
    "\n",
    "if CULL_NON_ALPHA:\n",
    "    print('Culling all non-alpha tokens...')\n",
    "    # then CULL by alpha\n",
    "    filtered_tokenized_texts = [[token for token in text if token.token.isalpha()]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_SHORT_TOKENS:\n",
    "    filtered_tokenized_texts = [[token for token in text if len(token.token) >= MIN_TOKEN_LENGTH]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "if CULL_SLANG_IN_VERBS:\n",
    "    print('Culling terms ending with \\'in...')\n",
    "    filtered_tokenized_texts = [[token for token in text if not token.token.endswith('in')]\n",
    "             for text in filtered_tokenized_texts]\n",
    "    \n",
    "# finally convert back from the object into simple token strings\n",
    "filtered_tokenized_texts = [[token.token for token in text] for text in filtered_tokenized_texts]\n",
    "\n",
    "DEMO_WORDS_TO_PRINT = 50\n",
    "\n",
    "print('Demo document tokens:')\n",
    "print(filtered_tokenized_texts[0][:DEMO_WORDS_TO_PRINT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up our initial dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dictionary size : [2743]\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(filtered_tokenized_texts)\n",
    "print('Total dictionary size : [{}]'.format(len(dictionary.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But before we go on, let's see if we should filter our vocabulary of very common terms (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_dictionary_by_global_idf(corpus, tokenized_texts, tfidf_model, dictionary, min_global_idf_value, \n",
    "                                    print_low_value_words = True):\n",
    "    low_value_words = set()\n",
    "    low_value_ids = set()\n",
    "    for id in tfidf.id2word.keys():\n",
    "        word = tfidf.id2word[id]\n",
    "        global_tf = tfidf.dfs[id]\n",
    "        global_idf = tfidf.idfs[id]\n",
    "        if global_idf < min_global_idf_value:\n",
    "            low_value_words.add(word)\n",
    "            low_value_ids.add(id)\n",
    "            \n",
    "    print('Total number of words removed : [{}]'.format(len(low_value_words)))\n",
    "\n",
    "    if print_low_value_words:\n",
    "        print('Here are all the words we will remove from our dictionary by global IDF')\n",
    "        print(sorted(list(low_value_words)))\n",
    "        \n",
    "        low_value_word_percent = len(low_value_words) / float(len(dictionary))\n",
    "\n",
    "        print('Total low value words : {0}'.format(len(low_value_words)))\n",
    "        print('Total low value Total Vocab percent : {0}'.format(low_value_word_percent))\n",
    "        \n",
    "    # now we can filter\n",
    "    dictionary.filter_tokens(bad_ids=low_value_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE : This class was taken from an example here:\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/dtm_example.ipynb\n",
    "\n",
    "class Corpus(gensim.corpora.textcorpus.TextCorpus):\n",
    "\n",
    "    def get_texts(self):\n",
    "        # let's also make sure that all empty documents (no tokens) are not included \n",
    "        # NOTE : The DIM model (model = 'fixed') breaks if there are any documents with no tokens\n",
    "        # also, apparently the logic for checking this is any(not text), so let's try that\n",
    "        #print('Calling a version of get_texts() that should not allow empty/None lists...')\n",
    "        return [x for x in self.input]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length : 2185\n",
      "Total dictionary size BEFORE TF-IDF filtering : [2743]\n",
      "Total number of words removed : [257]\n",
      "Here are all the words we will remove from our dictionary by global IDF\n",
      "['another', 'around', 'away', 'baby', 'back', 'ball', 'bang', 'beach', 'behind', 'bitch', 'bitches', 'black', 'blast', 'block', 'blood', 'blow', 'body', 'bomb', 'boss', 'bout', 'boys', 'brothers', 'bust', 'call', 'care', 'case', 'cash', 'catch', 'cause', 'change', 'chest', 'chill', 'city', 'close', 'clothes', 'club', 'cold', 'control', 'cops', 'corner', 'could', 'crack', 'crazy', 'daddy', 'dance', 'days', 'deal', 'death', 'dick', 'dollar', 'door', 'dope', 'dough', 'dream', 'drink', 'drop', 'enough', 'every', 'everybody', 'everything', 'eyes', 'face', 'fact', 'fall', 'family', 'fast', 'feet', 'fight', 'fire', 'five', 'flip', 'floor', 'flow', 'fool', 'fools', 'forever', 'four', 'friend', 'friends', 'front', 'fuck', 'funk', 'game', 'gang', 'gangsta', 'ghetto', 'girl', 'girls', 'glock', 'gold', 'grab', 'grip', 'guess', 'guns', 'hair', 'half', 'hand', 'hands', 'hate', 'head', 'heart', 'hell', 'help', 'high', 'home', 'homie', 'homies', 'hope', 'house', 'inside', 'jack', 'kick', 'kids', 'kinda', 'ladies', 'life', 'light', 'lights', 'like', 'line', 'living', 'long', 'look', 'lord', 'mama', 'matter', 'might', 'million', 'mind', 'mine', 'momma', 'money', 'mother', 'motherfucker', 'motherfuckers', 'mouth', 'move', 'murder', 'music', 'must', 'name', 'need', 'next', 'nigga', 'niggas', 'niggaz', 'night', 'nine', 'nobody', 'none', 'nothing', 'number', 'open', 'outta', 'pack', 'paper', 'part', 'party', 'past', 'peace', 'people', 'phone', 'picture', 'piece', 'place', 'plan', 'play', 'player', 'plus', 'point', 'police', 'pound', 'punk', 'push', 'pussy', 'reason', 'record', 'remember', 'respect', 'rest', 'rhyme', 'ride', 'right', 'rock', 'rocks', 'roll', 'room', 'school', 'shine', 'shot', 'shots', 'show', 'sick', 'side', 'since', 'sleep', 'smoke', 'somebody', 'something', 'song', 'soon', 'sound', 'spit', 'spot', 'stack', 'stand', 'star', 'start', 'state', 'step', 'still', 'straight', 'street', 'streets', 'style', 'sure', 'talk', 'team', 'test', 'thang', 'thing', 'things', 'though', 'thought', 'three', 'throw', 'thug', 'till', 'time', 'times', 'together', 'tonight', 'touch', 'town', 'track', 'trick', 'trip', 'trust', 'truth', 'turn', 'type', 'wake', 'watch', 'west', 'white', 'wild', 'wish', 'without', 'wonder', 'word', 'words', 'work', 'world', 'would', 'yeah', 'year', 'years']\n",
      "Total low value words : 257\n",
      "Total low value Total Vocab percent : 0.0936930368209989\n",
      "Dictionary(2486 unique tokens: ['adversaries', 'bars', 'bass', 'blade', 'boulevard']...)\n",
      "Dictionary(2743 unique tokens: ['adversaries', 'bars', 'bass', 'blade', 'boulevard']...)\n",
      "Total dictionary size AFTER TF-IDF filtering : [2486]\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filtered_tokenized_texts)\n",
    "\n",
    "# this is a corpus which has not yet been pruned so we can do meaningful TF-IDF calculations with it\n",
    "# NOTE that we call \"token.token\" since it is of class token and we want the string for the actual token\n",
    "unfiltered_corpus = [dictionary.doc2bow([token.token for token in tokens]) for tokens in tokenized_texts]\n",
    "\n",
    "tfidf = gensim.models.TfidfModel(unfiltered_corpus, id2word=dictionary)\n",
    "\n",
    "print('Corpus length : {}'.format(len(corpus)))\n",
    "\n",
    "if FILTER_DICTIONARY_BY_GLOBAL_IDF:\n",
    "    print('Total dictionary size BEFORE TF-IDF filtering : [{}]'.format(len(dictionary.keys())))\n",
    "    \n",
    "    filter_dictionary_by_global_idf(unfiltered_corpus, filtered_tokenized_texts, tfidf, dictionary, MIN_GLOBAL_IDF_VALUE)\n",
    "    print(dictionary)\n",
    "    print(corpus.dictionary)\n",
    "    # re-point the new dictionary for the corpus\n",
    "    corpus.dictionary = dictionary\n",
    "    \n",
    "    print('Total dictionary size AFTER TF-IDF filtering : [{}]'.format(len(corpus.dictionary.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training single-core model...\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "NUM_TOPICS = 8\n",
    "NUM_WORKERS = 6\n",
    "VANILLA_LDA_PASSES = 30\n",
    "\n",
    "TRAIN_MULTICORE_MODEL = False\n",
    "\n",
    "# train model\n",
    "lda = None\n",
    "if TRAIN_MULTICORE_MODEL:\n",
    "    print('Training multicore model...')\n",
    "    lda = gensim.models.LdaMulticore(corpus, \n",
    "                                     id2word = dictionary, \n",
    "                                     num_topics = NUM_TOPICS, \n",
    "                                     workers = NUM_WORKERS, \n",
    "                                     passes = VANILLA_LDA_PASSES)\n",
    "else:\n",
    "    print('Training single-core model...')\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus, \n",
    "                                     id2word = dictionary, \n",
    "                                     num_topics = NUM_TOPICS,\n",
    "                                         passes = VANILLA_LDA_PASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"hide\" + 0.013*\"homeboys\" + 0.013*\"outlaw\" + 0.012*\"toss\" + 0.011*\"feelins\" + 0.011*\"tears\" + 0.010*\"pray\" + 0.009*\"drama\" + 0.008*\"grow\" + 0.008*\"ones\"'),\n",
       " (1,\n",
       "  '0.032*\"thugs\" + 0.024*\"swear\" + 0.018*\"soldiers\" + 0.012*\"dreams\" + 0.011*\"nation\" + 0.010*\"lady\" + 0.010*\"pistol\" + 0.009*\"children\" + 0.007*\"child\" + 0.007*\"rebel\"'),\n",
       " (2,\n",
       "  '0.021*\"hennessy\" + 0.015*\"exchange\" + 0.014*\"slang\" + 0.014*\"bottle\" + 0.013*\"boom\" + 0.011*\"hail\" + 0.010*\"role\" + 0.010*\"model\" + 0.009*\"funky\" + 0.009*\"pimp\"'),\n",
       " (3,\n",
       "  '0.011*\"brooklyn\" + 0.006*\"alright\" + 0.006*\"cream\" + 0.006*\"hype\" + 0.006*\"shorty\" + 0.005*\"microphone\" + 0.004*\"flipmode\" + 0.004*\"beef\" + 0.004*\"round\" + 0.004*\"water\"'),\n",
       " (4,\n",
       "  '0.063*\"enemies\" + 0.023*\"outlaw\" + 0.017*\"cards\" + 0.015*\"bail\" + 0.015*\"lane\" + 0.013*\"holla\" + 0.013*\"haha\" + 0.013*\"cappucino\" + 0.011*\"westside\" + 0.011*\"patient\"'),\n",
       " (5,\n",
       "  '0.015*\"chick\" + 0.012*\"cats\" + 0.011*\"playa\" + 0.009*\"eyez\" + 0.009*\"cars\" + 0.008*\"platinum\" + 0.007*\"children\" + 0.007*\"chicks\" + 0.007*\"self\" + 0.006*\"business\"'),\n",
       " (6,\n",
       "  '0.020*\"harlem\" + 0.018*\"chorus\" + 0.011*\"honey\" + 0.010*\"lets\" + 0.009*\"verse\" + 0.009*\"teardrops\" + 0.009*\"tour\" + 0.009*\"caskets\" + 0.009*\"cotton\" + 0.008*\"dopest\"'),\n",
       " (7,\n",
       "  '0.025*\"mansion\" + 0.015*\"cali\" + 0.014*\"california\" + 0.012*\"twyla\" + 0.012*\"smart\" + 0.010*\"hustle\" + 0.010*\"dear\" + 0.010*\"season\" + 0.009*\"video\" + 0.009*\"fortune\"')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have a model, let's store the topic inferences back into our dataframe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic assignment progress : [0/2185]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:337: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "c:\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic assignment progress : [1000/2185]\n",
      "Topic assignment progress : [2000/2185]\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(index_list):\n",
    "    if i % 1000 == 0:\n",
    "        print('Topic assignment progress : [{0}/{1}]'.format(i, len(index_list)))\n",
    "        \n",
    "    doc_bow = corpus.dictionary.doc2bow(filtered_tokenized_texts[i])\n",
    "       \n",
    "    document_topic_sparse_list = [0.0] * lda.num_topics\n",
    "    document_topics = lda.get_document_topics(doc_bow)\n",
    "    \n",
    "    # find the topics actually predicted for this document and store them into the array\n",
    "    for document_topic in document_topics:\n",
    "        topic_idx = document_topic[0]\n",
    "        topic_prob = document_topic[1]\n",
    "        document_topic_sparse_list[topic_idx] = topic_prob\n",
    "    \n",
    "    # now we can store these into a dataframe\n",
    "    for topic_idx, topic_prob in enumerate(document_topic_sparse_list):\n",
    "        topic_key = 'Topic_{0}'.format(format(topic_idx, '02d'))\n",
    "        #print(topic_key)\n",
    "        rap_df = rap_df.set_value(index, topic_key, topic_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Band</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Song</th>\n",
       "      <th>Coast</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Topic_00</th>\n",
       "      <th>Topic_01</th>\n",
       "      <th>Topic_02</th>\n",
       "      <th>Topic_03</th>\n",
       "      <th>Topic_04</th>\n",
       "      <th>Topic_05</th>\n",
       "      <th>Topic_06</th>\n",
       "      <th>Topic_07</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4122</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>His realm is a new horizon of cuts . They expa...</td>\n",
       "      <td>Dangerous</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.195859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274623</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4123</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Yo Yvette, there's a lot of rumors goin' aroun...</td>\n",
       "      <td>Dear Yvette</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.626720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4124</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>I seen this girl, walking down the block . I s...</td>\n",
       "      <td>I Can Give You More</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.104152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098234</td>\n",
       "      <td>0.231990</td>\n",
       "      <td>0.086853</td>\n",
       "      <td>0.348372</td>\n",
       "      <td>0.126360</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4125</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Look girl, I'm not gonna sing . 'Cause I just ...</td>\n",
       "      <td>I Want You</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519170</td>\n",
       "      <td>0.096339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175167</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4126</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Yo man, I got this def Rolex watch, man . A hu...</td>\n",
       "      <td>That's a Lie</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065824</td>\n",
       "      <td>0.059803</td>\n",
       "      <td>0.323381</td>\n",
       "      <td>0.140912</td>\n",
       "      <td>0.241826</td>\n",
       "      <td>0.152852</td>\n",
       "      <td>0.013507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>He boy, I've seen ya . Think ya rockin' it on ...</td>\n",
       "      <td>You Can't Dance</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272891</td>\n",
       "      <td>0.082022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498346</td>\n",
       "      <td>0.125872</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>The momentum of this party can only increase ....</td>\n",
       "      <td>You'll Rock</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.097766</td>\n",
       "      <td>0.045925</td>\n",
       "      <td>0.091243</td>\n",
       "      <td>0.434779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188963</td>\n",
       "      <td>0.137149</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4129</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>That's right y'all this is the LL Cool J party...</td>\n",
       "      <td>.357 - Break It on Down</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155036</td>\n",
       "      <td>0.470562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315771</td>\n",
       "      <td>0.050926</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4130</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>I'm the Ladies Love, legend in leather . Long ...</td>\n",
       "      <td>Ahh, Let's Get Ill</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.070722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125319</td>\n",
       "      <td>0.445402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062369</td>\n",
       "      <td>0.288368</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Knuckleheads spreadin' gossip all over town  ....</td>\n",
       "      <td>The Breakthrough</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.123490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150264</td>\n",
       "      <td>0.549399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054120</td>\n",
       "      <td>0.116949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4132</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>I know a fat girl, she wears a orange skirt  ....</td>\n",
       "      <td>The Bristol Hotel</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.611336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320626</td>\n",
       "      <td>0.047864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>L.L. Cool J . Serving em well . And as you all...</td>\n",
       "      <td>The Do Wop</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087117</td>\n",
       "      <td>0.135224</td>\n",
       "      <td>0.568295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173994</td>\n",
       "      <td>0.030898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4134</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>G, G, G, G, G, G Get .  . Down, to the rhythm ...</td>\n",
       "      <td>Get Down</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.035337</td>\n",
       "      <td>0.128032</td>\n",
       "      <td>0.158086</td>\n",
       "      <td>0.298556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158446</td>\n",
       "      <td>0.216214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4135</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>1, 2, 3 'o' clock 4 'o' clock rock! . 5, 6, 7 ...</td>\n",
       "      <td>Go Cut Creator Go</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.148112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190039</td>\n",
       "      <td>0.151441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065139</td>\n",
       "      <td>0.437279</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4136</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>I met this new girl with big juicy lips . And ...</td>\n",
       "      <td>Kanday</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166277</td>\n",
       "      <td>0.371757</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4137</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Do you remember the first time you fell in lov...</td>\n",
       "      <td>You're My Heart</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.281236</td>\n",
       "      <td>0.166970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391421</td>\n",
       "      <td>0.048626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101860</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Yo \"Hello 1-900-LL Cool J?\" . Yeah whats up? \"...</td>\n",
       "      <td>1-900 L.L. Cool J</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.048308</td>\n",
       "      <td>0.124165</td>\n",
       "      <td>0.066114</td>\n",
       "      <td>0.555410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082088</td>\n",
       "      <td>0.117954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4139</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>I was at the mall, sippin' on a milkshake . Pl...</td>\n",
       "      <td>Big Ole Butt</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531688</td>\n",
       "      <td>0.046534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4140</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Come on, come on, come on, come on . Change yo...</td>\n",
       "      <td>Change Your Ways</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331007</td>\n",
       "      <td>0.223190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145583</td>\n",
       "      <td>0.168792</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Yeah, . Yeah I like that guitar man, yeah . Yo...</td>\n",
       "      <td>Clap Your Hands</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025315</td>\n",
       "      <td>0.210643</td>\n",
       "      <td>0.428999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4142</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Yeah, yeah, checkin' 'em out . Special shout-o...</td>\n",
       "      <td>Def Jam in the Motherland</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.027554</td>\n",
       "      <td>0.318843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379484</td>\n",
       "      <td>0.039317</td>\n",
       "      <td>0.083778</td>\n",
       "      <td>0.147171</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4143</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Just like Pauline you all-in .  . Oh, . Brace ...</td>\n",
       "      <td>Droppin' Em</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082515</td>\n",
       "      <td>0.090773</td>\n",
       "      <td>0.518561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Yeah . Funky! Uh . Yeah . Yo, let me tell you ...</td>\n",
       "      <td>Fast Peg</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265888</td>\n",
       "      <td>0.270143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245007</td>\n",
       "      <td>0.103367</td>\n",
       "      <td>0.098525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>I'm going back to Cali, Cali, Cali . I'm going...</td>\n",
       "      <td>Going Back to Cali</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045727</td>\n",
       "      <td>0.834055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4146</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>You're the type of guy that can't control your...</td>\n",
       "      <td>I'm That Type of Guy</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093462</td>\n",
       "      <td>0.120394</td>\n",
       "      <td>0.458881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4147</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Let me tell you somethin' about an a-b-c style...</td>\n",
       "      <td>It Gets No Rougher</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.037818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118742</td>\n",
       "      <td>0.579407</td>\n",
       "      <td>0.064555</td>\n",
       "      <td>0.101955</td>\n",
       "      <td>0.095020</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Yeah you know what I'm saying?  . It's real fu...</td>\n",
       "      <td>Jealous</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037815</td>\n",
       "      <td>0.517271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139197</td>\n",
       "      <td>0.153050</td>\n",
       "      <td>0.144842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4149</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>(They're jingling, baby) Go 'head, baby . (The...</td>\n",
       "      <td>Jingling Baby</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178222</td>\n",
       "      <td>0.173723</td>\n",
       "      <td>0.288220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4150</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>Check this . I excel, they fell, I said, well,...</td>\n",
       "      <td>Nitro</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229144</td>\n",
       "      <td>0.588941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132011</td>\n",
       "      <td>0.043869</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4151</th>\n",
       "      <td>LL Cool J</td>\n",
       "      <td>One shot . Oh lord . You know, it's gonna be a...</td>\n",
       "      <td>One Shot at Love</td>\n",
       "      <td>East</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.423692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.186780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261551</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Now you called my house and you said come over...</td>\n",
       "      <td>Take It Off</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076010</td>\n",
       "      <td>0.892711</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261552</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>[only vocal parts feat. MC Lyte are shown] .  ...</td>\n",
       "      <td>When in Love</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.960185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261553</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Ain't no other, this is me and this is it . Do...</td>\n",
       "      <td>Ain't No Other</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>0.010437</td>\n",
       "      <td>0.010422</td>\n",
       "      <td>0.927020</td>\n",
       "      <td>0.010422</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>0.010420</td>\n",
       "      <td>0.010423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261554</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>(Brooklyn, Brooklyn, Brooklyn...) .  . [ VERSE...</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.823249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161112</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261555</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Step 1 to the 2\\t (Repeat x 4) . Yeah . Rrrah ...</td>\n",
       "      <td>Hard Copy</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.941654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261556</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Gotta who? Gotta have a what? . Gotta what yo,...</td>\n",
       "      <td>Ruffneck</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059263</td>\n",
       "      <td>0.677552</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261557</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>In comes the boom to the bam here I stands . M...</td>\n",
       "      <td>What's My Name Yo</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.656131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317986</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261558</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>To the L, to the Y and the T to the E . So get...</td>\n",
       "      <td>Cold Rock a Party</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.915012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261559</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Got a new gig, here you come again kid . Fresh...</td>\n",
       "      <td>Druglord Superstar</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.140529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.836009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261560</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Hook: . Every day I need my shit done in a spe...</td>\n",
       "      <td>Everyday</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.147625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264305</td>\n",
       "      <td>0.563995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261561</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Hook: .  . Have you ever, ever in long living ...</td>\n",
       "      <td>Have U Ever</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261562</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Yeah! Uh! Uh yeah! Wooh! . Smooth, better than...</td>\n",
       "      <td>One on One</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077546</td>\n",
       "      <td>0.448094</td>\n",
       "      <td>0.443083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261563</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>TRG, we making that cream. . People get fooled...</td>\n",
       "      <td>TRG (The Rap Game)</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.975676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261564</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>be\\t-Boy, where the fuck you at?  . I been loo...</td>\n",
       "      <td>Keep on Keepin' On [T.V. Track with Ad Libs]</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.949676</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261565</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>(chorus) (G. Salah) . Let's break it down... ....</td>\n",
       "      <td>Break It Down</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156939</td>\n",
       "      <td>0.302816</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261566</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Loving you madly will be forever.  . I see the...</td>\n",
       "      <td>Closer</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.958314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261567</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Make it last long, uha, uh .  . Here's a situa...</td>\n",
       "      <td>Give Me What I Want</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.974984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261568</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>[Intro] .  . Hehah, yeah...yeah...yeah...yeah....</td>\n",
       "      <td>I Can't Make a Mistake</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.919739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261569</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>featuring Missy 'Misdemeanor' Elliott  .  .  ....</td>\n",
       "      <td>In My Business</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171184</td>\n",
       "      <td>0.806058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261570</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Our love is old school like Mary Jane's  . Bos...</td>\n",
       "      <td>It's All Yours</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261571</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Intro (Inaya) . Mmmm, mmmm . Oh oh, oh oh . Mm...</td>\n",
       "      <td>Party Goin' On</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.971746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261572</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>featuring Missy 'Misdemeanor' Elliott  Mocha  ...</td>\n",
       "      <td>Want What I Got</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.971753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261573</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>B-boy, where the fuck you at? . I been looking...</td>\n",
       "      <td>Keep on, Keepin' On</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.948404</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261574</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Yeah, I heard somebody say fire . Why'all want...</td>\n",
       "      <td>Fire</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261575</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Yeah, God said Lyte from the beginning of time...</td>\n",
       "      <td>God Said Lyte</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261576</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>[gun cocked back] .  . [Eminem] . Nah, we ain'...</td>\n",
       "      <td>Outro</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261577</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Lytro, yeah, hm, yo . Yeah, fuck the rest be, ...</td>\n",
       "      <td>Ride Wit Me</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.984364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261578</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>You lookin' good boy, you got that fire now . ...</td>\n",
       "      <td>U Got It</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261579</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>[Brandy] . Oh yeah ooh yeah . I would like to ...</td>\n",
       "      <td>I Wanna Be Down [Remix]</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.919393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261580</th>\n",
       "      <td>MC Lyte</td>\n",
       "      <td>Oh hey hey well well well . Do you mind if I t...</td>\n",
       "      <td>Maybe I Deserve</td>\n",
       "      <td>East</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.017910</td>\n",
       "      <td>0.017872</td>\n",
       "      <td>0.017875</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>0.186014</td>\n",
       "      <td>0.017868</td>\n",
       "      <td>0.706722</td>\n",
       "      <td>0.017863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2185 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Band                                             Lyrics  \\\n",
       "4122    LL Cool J  His realm is a new horizon of cuts . They expa...   \n",
       "4123    LL Cool J  Yo Yvette, there's a lot of rumors goin' aroun...   \n",
       "4124    LL Cool J  I seen this girl, walking down the block . I s...   \n",
       "4125    LL Cool J  Look girl, I'm not gonna sing . 'Cause I just ...   \n",
       "4126    LL Cool J  Yo man, I got this def Rolex watch, man . A hu...   \n",
       "4127    LL Cool J  He boy, I've seen ya . Think ya rockin' it on ...   \n",
       "4128    LL Cool J  The momentum of this party can only increase ....   \n",
       "4129    LL Cool J  That's right y'all this is the LL Cool J party...   \n",
       "4130    LL Cool J  I'm the Ladies Love, legend in leather . Long ...   \n",
       "4131    LL Cool J  Knuckleheads spreadin' gossip all over town  ....   \n",
       "4132    LL Cool J  I know a fat girl, she wears a orange skirt  ....   \n",
       "4133    LL Cool J  L.L. Cool J . Serving em well . And as you all...   \n",
       "4134    LL Cool J  G, G, G, G, G, G Get .  . Down, to the rhythm ...   \n",
       "4135    LL Cool J  1, 2, 3 'o' clock 4 'o' clock rock! . 5, 6, 7 ...   \n",
       "4136    LL Cool J  I met this new girl with big juicy lips . And ...   \n",
       "4137    LL Cool J  Do you remember the first time you fell in lov...   \n",
       "4138    LL Cool J  Yo \"Hello 1-900-LL Cool J?\" . Yeah whats up? \"...   \n",
       "4139    LL Cool J  I was at the mall, sippin' on a milkshake . Pl...   \n",
       "4140    LL Cool J  Come on, come on, come on, come on . Change yo...   \n",
       "4141    LL Cool J  Yeah, . Yeah I like that guitar man, yeah . Yo...   \n",
       "4142    LL Cool J  Yeah, yeah, checkin' 'em out . Special shout-o...   \n",
       "4143    LL Cool J  Just like Pauline you all-in .  . Oh, . Brace ...   \n",
       "4144    LL Cool J  Yeah . Funky! Uh . Yeah . Yo, let me tell you ...   \n",
       "4145    LL Cool J  I'm going back to Cali, Cali, Cali . I'm going...   \n",
       "4146    LL Cool J  You're the type of guy that can't control your...   \n",
       "4147    LL Cool J  Let me tell you somethin' about an a-b-c style...   \n",
       "4148    LL Cool J  Yeah you know what I'm saying?  . It's real fu...   \n",
       "4149    LL Cool J  (They're jingling, baby) Go 'head, baby . (The...   \n",
       "4150    LL Cool J  Check this . I excel, they fell, I said, well,...   \n",
       "4151    LL Cool J  One shot . Oh lord . You know, it's gonna be a...   \n",
       "...           ...                                                ...   \n",
       "261551    MC Lyte  Now you called my house and you said come over...   \n",
       "261552    MC Lyte  [only vocal parts feat. MC Lyte are shown] .  ...   \n",
       "261553    MC Lyte  Ain't no other, this is me and this is it . Do...   \n",
       "261554    MC Lyte  (Brooklyn, Brooklyn, Brooklyn...) .  . [ VERSE...   \n",
       "261555    MC Lyte  Step 1 to the 2\\t (Repeat x 4) . Yeah . Rrrah ...   \n",
       "261556    MC Lyte  Gotta who? Gotta have a what? . Gotta what yo,...   \n",
       "261557    MC Lyte  In comes the boom to the bam here I stands . M...   \n",
       "261558    MC Lyte  To the L, to the Y and the T to the E . So get...   \n",
       "261559    MC Lyte  Got a new gig, here you come again kid . Fresh...   \n",
       "261560    MC Lyte  Hook: . Every day I need my shit done in a spe...   \n",
       "261561    MC Lyte  Hook: .  . Have you ever, ever in long living ...   \n",
       "261562    MC Lyte  Yeah! Uh! Uh yeah! Wooh! . Smooth, better than...   \n",
       "261563    MC Lyte  TRG, we making that cream. . People get fooled...   \n",
       "261564    MC Lyte  be\\t-Boy, where the fuck you at?  . I been loo...   \n",
       "261565    MC Lyte  (chorus) (G. Salah) . Let's break it down... ....   \n",
       "261566    MC Lyte  Loving you madly will be forever.  . I see the...   \n",
       "261567    MC Lyte  Make it last long, uha, uh .  . Here's a situa...   \n",
       "261568    MC Lyte  [Intro] .  . Hehah, yeah...yeah...yeah...yeah....   \n",
       "261569    MC Lyte  featuring Missy 'Misdemeanor' Elliott  .  .  ....   \n",
       "261570    MC Lyte  Our love is old school like Mary Jane's  . Bos...   \n",
       "261571    MC Lyte  Intro (Inaya) . Mmmm, mmmm . Oh oh, oh oh . Mm...   \n",
       "261572    MC Lyte  featuring Missy 'Misdemeanor' Elliott  Mocha  ...   \n",
       "261573    MC Lyte  B-boy, where the fuck you at? . I been looking...   \n",
       "261574    MC Lyte  Yeah, I heard somebody say fire . Why'all want...   \n",
       "261575    MC Lyte  Yeah, God said Lyte from the beginning of time...   \n",
       "261576    MC Lyte  [gun cocked back] .  . [Eminem] . Nah, we ain'...   \n",
       "261577    MC Lyte  Lytro, yeah, hm, yo . Yeah, fuck the rest be, ...   \n",
       "261578    MC Lyte  You lookin' good boy, you got that fire now . ...   \n",
       "261579    MC Lyte  [Brandy] . Oh yeah ooh yeah . I would like to ...   \n",
       "261580    MC Lyte  Oh hey hey well well well . Do you mind if I t...   \n",
       "\n",
       "                                                Song Coast  Gender  Topic_00  \\\n",
       "4122                                       Dangerous  East    Male  0.195859   \n",
       "4123                                     Dear Yvette  East    Male  0.000000   \n",
       "4124                             I Can Give You More  East    Male  0.104152   \n",
       "4125                                      I Want You  East    Male  0.000000   \n",
       "4126                                    That's a Lie  East    Male  0.000000   \n",
       "4127                                 You Can't Dance  East    Male  0.000000   \n",
       "4128                                     You'll Rock  East    Male  0.097766   \n",
       "4129                         .357 - Break It on Down  East    Male  0.000000   \n",
       "4130                              Ahh, Let's Get Ill  East    Male  0.070722   \n",
       "4131                                The Breakthrough  East    Male  0.123490   \n",
       "4132                               The Bristol Hotel  East    Male  0.000000   \n",
       "4133                                      The Do Wop  East    Male  0.000000   \n",
       "4134                                        Get Down  East    Male  0.035337   \n",
       "4135                               Go Cut Creator Go  East    Male  0.148112   \n",
       "4136                                          Kanday  East    Male  0.000000   \n",
       "4137                                 You're My Heart  East    Male  0.281236   \n",
       "4138                               1-900 L.L. Cool J  East    Male  0.048308   \n",
       "4139                                    Big Ole Butt  East    Male  0.000000   \n",
       "4140                                Change Your Ways  East    Male  0.121000   \n",
       "4141                                 Clap Your Hands  East    Male  0.000000   \n",
       "4142                       Def Jam in the Motherland  East    Male  0.027554   \n",
       "4143                                     Droppin' Em  East    Male  0.000000   \n",
       "4144                                        Fast Peg  East    Male  0.000000   \n",
       "4145                              Going Back to Cali  East    Male  0.000000   \n",
       "4146                            I'm That Type of Guy  East    Male  0.000000   \n",
       "4147                              It Gets No Rougher  East    Male  0.037818   \n",
       "4148                                         Jealous  East    Male  0.000000   \n",
       "4149                                   Jingling Baby  East    Male  0.000000   \n",
       "4150                                           Nitro  East    Male  0.000000   \n",
       "4151                                One Shot at Love  East    Male  0.423692   \n",
       "...                                              ...   ...     ...       ...   \n",
       "261551                                   Take It Off  East  Female  0.000000   \n",
       "261552                                  When in Love  East  Female  0.960185   \n",
       "261553                                Ain't No Other  East  Female  0.010435   \n",
       "261554                                      Brooklyn  East  Female  0.000000   \n",
       "261555                                     Hard Copy  East  Female  0.000000   \n",
       "261556                                      Ruffneck  East  Female  0.000000   \n",
       "261557                             What's My Name Yo  East  Female  0.000000   \n",
       "261558                             Cold Rock a Party  East  Female  0.000000   \n",
       "261559                            Druglord Superstar  East  Female  0.140529   \n",
       "261560                                      Everyday  East  Female  0.147625   \n",
       "261561                                   Have U Ever  East  Female  0.000000   \n",
       "261562                                    One on One  East  Female  0.000000   \n",
       "261563                            TRG (The Rap Game)  East  Female  0.000000   \n",
       "261564  Keep on Keepin' On [T.V. Track with Ad Libs]  East  Female  0.000000   \n",
       "261565                                 Break It Down  East  Female  0.000000   \n",
       "261566                                        Closer  East  Female  0.958314   \n",
       "261567                           Give Me What I Want  East  Female  0.000000   \n",
       "261568                        I Can't Make a Mistake  East  Female  0.000000   \n",
       "261569                                In My Business  East  Female  0.000000   \n",
       "261570                                It's All Yours  East  Female  0.000000   \n",
       "261571                                Party Goin' On  East  Female  0.000000   \n",
       "261572                               Want What I Got  East  Female  0.000000   \n",
       "261573                           Keep on, Keepin' On  East  Female  0.000000   \n",
       "261574                                          Fire  East  Female  0.000000   \n",
       "261575                                 God Said Lyte  East  Female  0.000000   \n",
       "261576                                         Outro  East  Female  0.000000   \n",
       "261577                                   Ride Wit Me  East  Female  0.000000   \n",
       "261578                                      U Got It  East  Female  0.000000   \n",
       "261579                       I Wanna Be Down [Remix]  East  Female  0.000000   \n",
       "261580                               Maybe I Deserve  East  Female  0.017910   \n",
       "\n",
       "        Topic_01  Topic_02  Topic_03  Topic_04  Topic_05  Topic_06  Topic_07  \n",
       "4122    0.000000  0.274623  0.369565  0.000000  0.145649  0.000000  0.000000  \n",
       "4123    0.109231  0.000000  0.136776  0.000000  0.122110  0.000000  0.626720  \n",
       "4124    0.000000  0.098234  0.231990  0.086853  0.348372  0.126360  0.000000  \n",
       "4125    0.202065  0.000000  0.519170  0.096339  0.000000  0.175167  0.000000  \n",
       "4126    0.065824  0.059803  0.323381  0.140912  0.241826  0.152852  0.013507  \n",
       "4127    0.000000  0.272891  0.082022  0.000000  0.498346  0.125872  0.000000  \n",
       "4128    0.045925  0.091243  0.434779  0.000000  0.188963  0.137149  0.000000  \n",
       "4129    0.000000  0.155036  0.470562  0.000000  0.315771  0.050926  0.000000  \n",
       "4130    0.000000  0.125319  0.445402  0.000000  0.062369  0.288368  0.000000  \n",
       "4131    0.000000  0.150264  0.549399  0.000000  0.054120  0.116949  0.000000  \n",
       "4132    0.000000  0.000000  0.611336  0.000000  0.000000  0.320626  0.047864  \n",
       "4133    0.087117  0.135224  0.568295  0.000000  0.000000  0.173994  0.030898  \n",
       "4134    0.128032  0.158086  0.298556  0.000000  0.158446  0.216214  0.000000  \n",
       "4135    0.000000  0.190039  0.151441  0.000000  0.065139  0.437279  0.000000  \n",
       "4136    0.077062  0.000000  0.373257  0.000000  0.166277  0.371757  0.000000  \n",
       "4137    0.166970  0.000000  0.391421  0.048626  0.000000  0.101860  0.000000  \n",
       "4138    0.124165  0.066114  0.555410  0.000000  0.000000  0.082088  0.117954  \n",
       "4139    0.190862  0.000000  0.225016  0.000000  0.000000  0.531688  0.046534  \n",
       "4140    0.000000  0.331007  0.223190  0.000000  0.145583  0.168792  0.000000  \n",
       "4141    0.025315  0.210643  0.428999  0.000000  0.229869  0.000000  0.100477  \n",
       "4142    0.318843  0.000000  0.379484  0.039317  0.083778  0.147171  0.000000  \n",
       "4143    0.082515  0.090773  0.518561  0.000000  0.238884  0.000000  0.060920  \n",
       "4144    0.000000  0.265888  0.270143  0.000000  0.245007  0.103367  0.098525  \n",
       "4145    0.000000  0.107183  0.000000  0.000000  0.000000  0.045727  0.834055  \n",
       "4146    0.093462  0.120394  0.458881  0.000000  0.154105  0.000000  0.159249  \n",
       "4147    0.000000  0.118742  0.579407  0.064555  0.101955  0.095020  0.000000  \n",
       "4148    0.000000  0.037815  0.517271  0.000000  0.139197  0.153050  0.144842  \n",
       "4149    0.178222  0.173723  0.288220  0.000000  0.284919  0.000000  0.066380  \n",
       "4150    0.000000  0.229144  0.588941  0.000000  0.132011  0.043869  0.000000  \n",
       "4151    0.000000  0.000000  0.188682  0.000000  0.179071  0.000000  0.186780  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "261551  0.000000  0.000000  0.000000  0.000000  0.076010  0.892711  0.000000  \n",
       "261552  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "261553  0.010437  0.010422  0.927020  0.010422  0.010421  0.010420  0.010423  \n",
       "261554  0.000000  0.000000  0.823249  0.000000  0.000000  0.161112  0.000000  \n",
       "261555  0.000000  0.941654  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "261556  0.000000  0.000000  0.244786  0.000000  0.059263  0.677552  0.000000  \n",
       "261557  0.000000  0.656131  0.000000  0.000000  0.000000  0.317986  0.000000  \n",
       "261558  0.059109  0.000000  0.915012  0.000000  0.000000  0.000000  0.000000  \n",
       "261559  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.836009  \n",
       "261560  0.000000  0.000000  0.000000  0.000000  0.000000  0.264305  0.563995  \n",
       "261561  0.620725  0.000000  0.360501  0.000000  0.000000  0.000000  0.000000  \n",
       "261562  0.000000  0.000000  0.000000  0.000000  0.077546  0.448094  0.443083  \n",
       "261563  0.000000  0.000000  0.975676  0.000000  0.000000  0.000000  0.000000  \n",
       "261564  0.000000  0.000000  0.000000  0.031087  0.000000  0.949676  0.000000  \n",
       "261565  0.000000  0.000000  0.525345  0.000000  0.156939  0.302816  0.000000  \n",
       "261566  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "261567  0.000000  0.000000  0.974984  0.000000  0.000000  0.000000  0.000000  \n",
       "261568  0.000000  0.000000  0.057518  0.000000  0.000000  0.000000  0.919739  \n",
       "261569  0.000000  0.000000  0.000000  0.171184  0.806058  0.000000  0.000000  \n",
       "261570  0.000000  0.000000  0.000000  0.969801  0.000000  0.000000  0.000000  \n",
       "261571  0.000000  0.000000  0.971746  0.000000  0.000000  0.000000  0.000000  \n",
       "261572  0.000000  0.000000  0.000000  0.000000  0.971753  0.000000  0.000000  \n",
       "261573  0.000000  0.000000  0.000000  0.031852  0.000000  0.948404  0.000000  \n",
       "261574  0.000000  0.000000  0.000000  0.000000  0.982825  0.000000  0.000000  \n",
       "261575  0.000000  0.000000  0.000000  0.000000  0.985160  0.000000  0.000000  \n",
       "261576  0.282121  0.000000  0.707868  0.000000  0.000000  0.000000  0.000000  \n",
       "261577  0.000000  0.984364  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "261578  0.000000  0.969810  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "261579  0.000000  0.919393  0.000000  0.000000  0.000000  0.000000  0.053798  \n",
       "261580  0.017872  0.017875  0.017876  0.186014  0.017868  0.706722  0.017863  \n",
       "\n",
       "[2185 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(rap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now that we have a topic model, let's do some comparison by stratifying by various attributes (i.e. East/West coast) to see how the topics vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_average_topic_by_column(df, col_name, lda_model):\n",
    "    unique_col_values = df[col_name].unique()\n",
    "\n",
    "    column_topic_dicts = []\n",
    "    for unique_col_value in unique_col_values:\n",
    "        print('Preparing topic values for column [{0} == {1}]'.format(col_name, unique_col_value))\n",
    "        \n",
    "        column_topic_dict = {}\n",
    "        column_topic_dict[col_name] = unique_col_value\n",
    "        \n",
    "        # let's loop through topics and get average values\n",
    "        for topic_idx in range(lda_model.num_topics):\n",
    "            print('Preparing values for Topic : {}'.format(topic_idx))\n",
    "            topic_key = 'Topic_{0}'.format(format(topic_idx, '02d'))\n",
    "            topic_probs = df[df[col_name] == unique_col_value][topic_key]\n",
    "            avg_topic_probs = np.mean(topic_probs)\n",
    "            \n",
    "            column_topic_dict[topic_key] = avg_topic_probs\n",
    "            \n",
    "        column_topic_dicts.append(column_topic_dict)\n",
    "        \n",
    "    column_topic_df = pd.DataFrame(column_topic_dicts)\n",
    "    print(column_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing topic values for column [Coast == East]\n",
      "Preparing values for Topic : 0\n",
      "Preparing values for Topic : 1\n",
      "Preparing values for Topic : 2\n",
      "Preparing values for Topic : 3\n",
      "Preparing values for Topic : 4\n",
      "Preparing values for Topic : 5\n",
      "Preparing values for Topic : 6\n",
      "Preparing values for Topic : 7\n",
      "Preparing topic values for column [Coast == West]\n",
      "Preparing values for Topic : 0\n",
      "Preparing values for Topic : 1\n",
      "Preparing values for Topic : 2\n",
      "Preparing values for Topic : 3\n",
      "Preparing values for Topic : 4\n",
      "Preparing values for Topic : 5\n",
      "Preparing values for Topic : 6\n",
      "Preparing values for Topic : 7\n",
      "  Coast  Topic_00  Topic_01  Topic_02  Topic_03  Topic_04  Topic_05  Topic_06  \\\n",
      "0  East  0.069020  0.058886  0.107770  0.374730  0.037834  0.165887  0.110713   \n",
      "1  West  0.101592  0.086957  0.140313  0.271217  0.083314  0.127231  0.099052   \n",
      "\n",
      "   Topic_07  \n",
      "0  0.062874  \n",
      "1  0.077301  \n"
     ]
    }
   ],
   "source": [
    "visualize_average_topic_by_column(rap_df, 'Coast', lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing topic values for column [Gender == Male]\n",
      "Preparing values for Topic : 0\n",
      "Preparing values for Topic : 1\n",
      "Preparing values for Topic : 2\n",
      "Preparing values for Topic : 3\n",
      "Preparing values for Topic : 4\n",
      "Preparing values for Topic : 5\n",
      "Preparing values for Topic : 6\n",
      "Preparing values for Topic : 7\n",
      "Preparing topic values for column [Gender == Female]\n",
      "Preparing values for Topic : 0\n",
      "Preparing values for Topic : 1\n",
      "Preparing values for Topic : 2\n",
      "Preparing values for Topic : 3\n",
      "Preparing values for Topic : 4\n",
      "Preparing values for Topic : 5\n",
      "Preparing values for Topic : 6\n",
      "Preparing values for Topic : 7\n",
      "   Gender  Topic_00  Topic_01  Topic_02  Topic_03  Topic_04  Topic_05  \\\n",
      "0    Male  0.084924  0.075204  0.125315  0.322770  0.060834  0.147651   \n",
      "1  Female  0.080432  0.051100  0.108003  0.348026  0.048416  0.148036   \n",
      "\n",
      "   Topic_06  Topic_07  \n",
      "0  0.101303  0.070013  \n",
      "1  0.131707  0.067290  \n"
     ]
    }
   ],
   "source": [
    "visualize_average_topic_by_column(rap_df, 'Gender', lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
